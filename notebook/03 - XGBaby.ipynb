{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - XGBoost per Classificazione IncidentGrade\n",
    "\n",
    "**Obiettivo:** Predire il triage grade degli incidenti di cybersecurity (TruePositive, BenignPositive, FalsePositive)\n",
    "\n",
    "**Modello:** XGBoost Classifier\n",
    "\n",
    "**Metrica principale:** Macro F1-Score (come richiesto dal challenge GUIDE)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Caricamento dati processati\n",
    "2. Baseline model\n",
    "3. Ottimizzazione iperparametri\n",
    "4. Valutazione e analisi errori\n",
    "5. Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Caricamento Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Librerie importate con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i dataset processati\n",
    "print(\"Caricamento dataset processati...\")\n",
    "\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv')['IncidentGrade']\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv')['IncidentGrade']\n",
    "\n",
    "print(f\"\\nX_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nDistribuzione y_train:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nProporzioni:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti label in numeri per XGBoost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_target = LabelEncoder()\n",
    "y_train_encoded = le_target.fit_transform(y_train)\n",
    "y_test_encoded = le_target.transform(y_test)\n",
    "\n",
    "print(\"Mapping label:\")\n",
    "for i, label in enumerate(le_target.classes_):\n",
    "    print(f\"  {label} -> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training baseline XGBoost model...\\n\")\n",
    "\n",
    "# Baseline XGBoost con parametri di default\n",
    "baseline_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(le_target.classes_),\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Training\n",
    "baseline_model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    eval_set=[(X_test, y_test_encoded)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training completato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizioni baseline\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Metriche\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE MODEL - PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test_encoded, \n",
    "    y_pred_baseline, \n",
    "    target_names=le_target.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Metriche aggregate\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_baseline)\n",
    "macro_f1 = f1_score(y_test_encoded, y_pred_baseline, average='macro')\n",
    "macro_precision = precision_score(y_test_encoded, y_pred_baseline, average='macro')\n",
    "macro_recall = recall_score(y_test_encoded, y_pred_baseline, average='macro')\n",
    "\n",
    "print(f\"\\nACCURACY:          {accuracy:.4f}\")\n",
    "print(f\"MACRO F1-SCORE:    {macro_f1:.4f}\")\n",
    "print(f\"MACRO PRECISION:   {macro_precision:.4f}\")\n",
    "print(f\"MACRO RECALL:      {macro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_encoded, y_pred_baseline)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_target.classes_,\n",
    "            yticklabels=le_target.classes_)\n",
    "plt.title('Confusion Matrix - Baseline Model', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix normalizzata\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=le_target.classes_,\n",
    "            yticklabels=le_target.classes_)\n",
    "plt.title('Confusion Matrix (Normalizzata) - Baseline Model', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ottimizzazione Iperparametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola class weights per gestire lo sbilanciamento\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y_train_encoded\n",
    ")\n",
    "\n",
    "print(\"Sample weights calcolati per bilanciamento classi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training modello ottimizzato...\\n\")\n",
    "\n",
    "# XGBoost con parametri ottimizzati\n",
    "optimized_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(le_target.classes_),\n",
    "    eval_metric='mlogloss',\n",
    "    \n",
    "    # Parametri ottimizzati\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=300,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    \n",
    "    # Altro\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Training con sample weights\n",
    "optimized_model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    sample_weight=sample_weights,\n",
    "    eval_set=[(X_test, y_test_encoded)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training completato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizioni modello ottimizzato\n",
    "y_pred_optimized = optimized_model.predict(X_test)\n",
    "\n",
    "# Metriche\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZED MODEL - PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test_encoded, \n",
    "    y_pred_optimized, \n",
    "    target_names=le_target.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Metriche aggregate\n",
    "accuracy_opt = accuracy_score(y_test_encoded, y_pred_optimized)\n",
    "macro_f1_opt = f1_score(y_test_encoded, y_pred_optimized, average='macro')\n",
    "macro_precision_opt = precision_score(y_test_encoded, y_pred_optimized, average='macro')\n",
    "macro_recall_opt = recall_score(y_test_encoded, y_pred_optimized, average='macro')\n",
    "\n",
    "print(f\"\\nACCURACY:          {accuracy_opt:.4f}\")\n",
    "print(f\"MACRO F1-SCORE:    {macro_f1_opt:.4f}\")\n",
    "print(f\"MACRO PRECISION:   {macro_precision_opt:.4f}\")\n",
    "print(f\"MACRO RECALL:      {macro_recall_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto baseline vs ottimizzato\n",
    "comparison = pd.DataFrame({\n",
    "    'Metrica': ['Accuracy', 'Macro F1', 'Macro Precision', 'Macro Recall'],\n",
    "    'Baseline': [accuracy, macro_f1, macro_precision, macro_recall],\n",
    "    'Ottimizzato': [accuracy_opt, macro_f1_opt, macro_precision_opt, macro_recall_opt]\n",
    "})\n",
    "comparison['Miglioramento'] = ((comparison['Ottimizzato'] - comparison['Baseline']) / comparison['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFRONTO BASELINE vs OTTIMIZZATO\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix modello ottimizzato\n",
    "cm_opt = confusion_matrix(y_test_encoded, y_pred_optimized)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Confusion matrix assoluta\n",
    "sns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_target.classes_,\n",
    "            yticklabels=le_target.classes_,\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - Optimized Model', fontsize=14)\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Confusion matrix normalizzata\n",
    "cm_opt_norm = cm_opt.astype('float') / cm_opt.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_opt_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=le_target.classes_,\n",
    "            yticklabels=le_target.classes_,\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix (Normalizzata) - Optimized Model', fontsize=14)\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Esecuzione 5-Fold Stratified Cross-Validation...\\n\")\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation con macro F1\n",
    "cv_scores = cross_val_score(\n",
    "    optimized_model, \n",
    "    X_train, y_train_encoded,\n",
    "    cv=skf,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"Cross-Validation Macro F1 Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMedia:  {cv_scores.mean():.4f}\")\n",
    "print(f\"Std:    {cv_scores.std():.4f}\")\n",
    "print(f\"Min:    {cv_scores.min():.4f}\")\n",
    "print(f\"Max:    {cv_scores.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza CV scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Media: {cv_scores.mean():.4f}')\n",
    "plt.fill_between(range(1, len(cv_scores) + 1), \n",
    "                 cv_scores.mean() - cv_scores.std(), \n",
    "                 cv_scores.mean() + cv_scores.std(), \n",
    "                 alpha=0.2, color='gray')\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('Macro F1-Score', fontsize=12)\n",
    "plt.title('5-Fold Cross-Validation Scores', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': optimized_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Features più importanti:\")\n",
    "print(feature_importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_n = 20\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "plt.barh(range(top_n), top_features['Importance'], color='steelblue')\n",
    "plt.yticks(range(top_n), top_features['Feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title(f'Top {top_n} Feature Importance - XGBoost', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance cumulativa\n",
    "feature_importance['Cumulative_Importance'] = feature_importance['Importance'].cumsum() / feature_importance['Importance'].sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(feature_importance)), feature_importance['Cumulative_Importance'], linewidth=2)\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', label='80% Importanza')\n",
    "plt.axhline(y=0.9, color='orange', linestyle='--', label='90% Importanza')\n",
    "plt.xlabel('Numero di Features', fontsize=12)\n",
    "plt.ylabel('Importanza Cumulativa', fontsize=12)\n",
    "plt.title('Feature Importance Cumulativa', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quante features servono per 80% e 90%?\n",
    "n_features_80 = (feature_importance['Cumulative_Importance'] <= 0.8).sum()\n",
    "n_features_90 = (feature_importance['Cumulative_Importance'] <= 0.9).sum()\n",
    "\n",
    "print(f\"\\nFeatures necessarie per:\")\n",
    "print(f\"  80% importanza: {n_features_80}/{len(feature_importance)}\")\n",
    "print(f\"  90% importanza: {n_features_90}/{len(feature_importance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analisi Errori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica predizioni errate\n",
    "errors_mask = y_test_encoded != y_pred_optimized\n",
    "n_errors = errors_mask.sum()\n",
    "\n",
    "print(f\"Numero di errori: {n_errors} su {len(y_test_encoded)} ({n_errors/len(y_test_encoded)*100:.2f}%)\")\n",
    "\n",
    "# Analizza tipo di errori\n",
    "error_analysis = pd.DataFrame({\n",
    "    'True': le_target.inverse_transform(y_test_encoded[errors_mask]),\n",
    "    'Predicted': le_target.inverse_transform(y_pred_optimized[errors_mask])\n",
    "})\n",
    "\n",
    "print(\"\\nTipi di errori più comuni:\")\n",
    "error_counts = error_analysis.groupby(['True', 'Predicted']).size().sort_values(ascending=False)\n",
    "print(error_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza distribuzione errori\n",
    "plt.figure(figsize=(10, 6))\n",
    "error_counts_df = error_counts.reset_index()\n",
    "error_counts_df.columns = ['True', 'Predicted', 'Count']\n",
    "error_counts_df['Error_Type'] = error_counts_df['True'] + ' → ' + error_counts_df['Predicted']\n",
    "\n",
    "plt.barh(range(len(error_counts_df)), error_counts_df['Count'], color='coral')\n",
    "plt.yticks(range(len(error_counts_df)), error_counts_df['Error_Type'])\n",
    "plt.xlabel('Numero di Errori', fontsize=12)\n",
    "plt.ylabel('Tipo di Errore', fontsize=12)\n",
    "plt.title('Distribuzione degli Errori di Classificazione', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predizioni con Probabilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ottieni probabilità per analisi più approfondita\n",
    "y_pred_proba = optimized_model.predict_proba(X_test)\n",
    "\n",
    "# Analizza confidence delle predizioni\n",
    "max_proba = y_pred_proba.max(axis=1)\n",
    "\n",
    "print(\"Distribuzione confidence delle predizioni:\")\n",
    "print(f\"  Media:    {max_proba.mean():.4f}\")\n",
    "print(f\"  Mediana:  {np.median(max_proba):.4f}\")\n",
    "print(f\"  Min:      {max_proba.min():.4f}\")\n",
    "print(f\"  Max:      {max_proba.max():.4f}\")\n",
    "\n",
    "# Distribuzione per correttezza predizione\n",
    "correct_mask = y_test_encoded == y_pred_optimized\n",
    "print(f\"\\nConfidence predizioni corrette: {max_proba[correct_mask].mean():.4f}\")\n",
    "print(f\"Confidence predizioni errate:   {max_proba[~correct_mask].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza distribuzione confidence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram generale\n",
    "axes[0].hist(max_proba, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(max_proba.mean(), color='r', linestyle='--', label=f'Media: {max_proba.mean():.3f}')\n",
    "axes[0].set_xlabel('Confidence (Max Probability)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequenza', fontsize=12)\n",
    "axes[0].set_title('Distribuzione Confidence Predizioni', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# Comparison corrette vs errate\n",
    "axes[1].hist(max_proba[correct_mask], bins=30, alpha=0.6, label='Corrette', color='green', edgecolor='black')\n",
    "axes[1].hist(max_proba[~correct_mask], bins=30, alpha=0.6, label='Errate', color='red', edgecolor='black')\n",
    "axes[1].set_xlabel('Confidence (Max Probability)', fontsize=12)\n",
    "axes[1].set_ylabel('Frequenza', fontsize=12)\n",
    "axes[1].set_title('Confidence: Predizioni Corrette vs Errate', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Salvataggio Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva il modello ottimizzato\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Salva modello XGBoost\n",
    "optimized_model.save_model('../models/xgboost_optimized.json')\n",
    "\n",
    "# Salva anche con pickle per compatibilità\n",
    "with open('../models/xgboost_optimized.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized_model, f)\n",
    "\n",
    "# Salva label encoder\n",
    "with open('../models/label_encoder_target.pkl', 'wb') as f:\n",
    "    pickle.dump(le_target, f)\n",
    "\n",
    "# Salva feature importance\n",
    "feature_importance.to_csv('../models/feature_importance.csv', index=False)\n",
    "\n",
    "print(\"Modello e artifacts salvati in ../models/\")\n",
    "print(\"  - xgboost_optimized.json\")\n",
    "print(\"  - xgboost_optimized.pkl\")\n",
    "print(\"  - label_encoder_target.pkl\")\n",
    "print(\"  - feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Riepilogo Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RIEPILOGO FINALE - XGBOOST CLASSIFIER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDATASET:\")\n",
    "print(f\"  Training samples:  {len(X_train):,}\")\n",
    "print(f\"  Test samples:      {len(X_test):,}\")\n",
    "print(f\"  Features:          {X_train.shape[1]}\")\n",
    "print(f\"  Classi:            {len(le_target.classes_)} ({', '.join(le_target.classes_)})\")\n",
    "\n",
    "print(f\"\\nPERFORMANCE TEST SET:\")\n",
    "print(f\"  Accuracy:          {accuracy_opt:.4f}\")\n",
    "print(f\"  Macro F1-Score:    {macro_f1_opt:.4f} ⭐\")\n",
    "print(f\"  Macro Precision:   {macro_precision_opt:.4f}\")\n",
    "print(f\"  Macro Recall:      {macro_recall_opt:.4f}\")\n",
    "\n",
    "print(f\"\\nCROSS-VALIDATION (5-Fold):\")\n",
    "print(f\"  Macro F1 medio:    {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\nTOP 5 FEATURES:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['Feature']:30s} {row['Importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nPERFORMANCE PER CLASSE:\")\n",
    "for i, label in enumerate(le_target.classes_):\n",
    "    mask = y_test_encoded == i\n",
    "    class_f1 = f1_score(y_test_encoded[mask], y_pred_optimized[mask], average='binary', zero_division=0)\n",
    "    class_acc = accuracy_score(y_test_encoded[mask], y_pred_optimized[mask])\n",
    "    print(f\"  {label:20s} F1: {class_f1:.4f}  Accuracy: {class_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prossimi Passi\n",
    "\n",
    "**Possibili miglioramenti:**\n",
    "\n",
    "1. **Grid Search/Random Search:** Ottimizzazione sistematica iperparametri\n",
    "2. **Ensemble Methods:** Combinare XGBoost con LightGBM, CatBoost, Random Forest\n",
    "3. **Feature Engineering avanzato:** \n",
    "   - Interazioni tra features\n",
    "   - Aggregazioni temporali più sofisticate\n",
    "   - Features da testo (MITRE Techniques)\n",
    "4. **Gestione sbilanciamento:** SMOTE, ADASYN per oversampling\n",
    "5. **Threshold tuning:** Ottimizzare soglie di classificazione per massimizzare F1\n",
    "6. **Analisi errori:** Studio approfondito dei casi difficili\n",
    "7. **Calibrazione probabilità:** Platt scaling o isotonic regression\n",
    "\n",
    "**Per production:**\n",
    "- Pipeline completa con preprocessing\n",
    "- Monitoring delle performance nel tempo\n",
    "- A/B testing vs baseline\n",
    "- Explainability con SHAP values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
