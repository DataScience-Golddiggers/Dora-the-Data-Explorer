{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36916bc8",
   "metadata": {},
   "source": [
    "# Confronto Modelli su GUIDE Test Set\n",
    "\n",
    "**Obiettivo:** Confrontare le performance di tutti i modelli addestrati sul dataset di test reale GUIDE_Test.csv\n",
    "\n",
    "**Modelli:**\n",
    "1. XGBoost\n",
    "2. Random Forest\n",
    "3. K-Means (confronto con ground truth)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Caricamento e preprocessing GUIDE_Test.csv (stesso preprocessing del training)\n",
    "2. Caricamento modelli salvati\n",
    "3. Predizioni su test set reale\n",
    "4. Confronto performance e feature importance\n",
    "\n",
    "**Metriche:**\n",
    "- Test Accuracy\n",
    "- Test Macro F1-Score\n",
    "- Per-class F1-Score\n",
    "- Confusion Matrix\n",
    "- Feature Importance (top features comuni)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922a98e",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df368ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    silhouette_score,\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Librerie importate con successo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18c5ca",
   "metadata": {},
   "source": [
    "## 2. Preprocessing GUIDE_Test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ca547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Caricamento GUIDE_Test.csv...\")\n",
    "df_test = pd.read_csv('../data/GUIDE_Test.csv')\n",
    "\n",
    "print(f\"Dataset test caricato: {df_test.shape[0]:,} righe, {df_test.shape[1]} colonne\")\n",
    "\n",
    "# Rimuovi record senza target\n",
    "df_test = df_test[df_test['IncidentGrade'].notna()].copy()\n",
    "\n",
    "# Rimuovi colonne con >97% missing\n",
    "missing_pct = (df_test.isnull().sum() / len(df_test)) * 100\n",
    "cols_to_drop = missing_pct[missing_pct > 97].index.tolist()\n",
    "df_test = df_test.drop(columns=cols_to_drop)\n",
    "\n",
    "# Rimuovi stesse colonne del training\n",
    "cols_to_remove = [\n",
    "    'State', 'City', 'CountryCode',\n",
    "    'OSFamily', 'OSVersion', \n",
    "    'DeviceId', 'DeviceName',\n",
    "    'Sha256', 'FileName', 'FolderPath',\n",
    "    'AccountObjectId', 'AccountName', 'AccountSid', 'AccountUpn',\n",
    "    'IpAddress', 'Url', 'NetworkMessageId', 'EmailClusterId',\n",
    "    'RegistryKey', 'RegistryValueName', 'RegistryValueData',\n",
    "    'ApplicationId', 'ApplicationName', 'OAuthApplicationId',\n",
    "    'ThreatFamily', 'ResourceIdName', 'ResourceType', 'Roles'\n",
    "]\n",
    "\n",
    "cols_existing = [col for col in cols_to_remove if col in df_test.columns]\n",
    "if cols_existing:\n",
    "    df_test = df_test.drop(columns=cols_existing)\n",
    "\n",
    "# Rimuovi duplicati\n",
    "df_test = df_test.drop_duplicates(subset=['Id'], keep='first')\n",
    "\n",
    "print(f\"Dimensioni dopo pulizia: {df_test.shape}\")\n",
    "print(f\"Distribuzione IncidentGrade:\\n{df_test['IncidentGrade'].value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9174f4",
   "metadata": {},
   "source": [
    "## 3. Caricamento Modelli e Predizioni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica label encoders del training\n",
    "with open('../data/processed/label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "# Applica encoding categorico (usa gli stessi encoder del training)\n",
    "categorical_cols = X_test.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in label_encoders:\n",
    "        le = label_encoders[col]\n",
    "        # Gestisci valori non visti nel training\n",
    "        X_test[col] = X_test[col].apply(\n",
    "            lambda x: x if x in le.classes_ else 'Other'\n",
    "        )\n",
    "        # Se 'Other' non è nelle classi, aggiungi\n",
    "        if 'Other' not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, 'Other')\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    else:\n",
    "        # Colonna nuova non vista nel training\n",
    "        X_test[col] = 0\n",
    "\n",
    "# Gestisci missing values\n",
    "X_test = X_test.fillna(-999)\n",
    "\n",
    "# Assicurati che le colonne corrispondano a quelle del training\n",
    "X_train_sample = pd.read_csv('../data/processed/X_train.csv')\n",
    "training_cols = X_train_sample.columns.tolist()\n",
    "\n",
    "# Aggiungi colonne mancanti\n",
    "for col in training_cols:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "\n",
    "# Rimuovi colonne extra\n",
    "X_test = X_test[training_cols]\n",
    "\n",
    "print(f\"X_test finale: {X_test.shape}\")\n",
    "print(f\"Match con training: {list(X_test.columns) == training_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07245bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola durata e rinomina\n",
    "incident_agg['Duration_seconds'] = (\n",
    "    pd.to_datetime(incident_agg['Timestamp_max']) - \n",
    "    pd.to_datetime(incident_agg['Timestamp_min'])\n",
    ").dt.total_seconds()\n",
    "\n",
    "rename_map = {\n",
    "    'AlertId_nunique': 'NumAlerts',\n",
    "    'Id_count': 'NumEvidences',\n",
    "    'EntityType_nunique': 'NumEntityTypes',\n",
    "    'EvidenceRole_nunique': 'NumEvidenceRoles',\n",
    "    'Hour_min': 'Hour_First',\n",
    "    'Hour_max': 'Hour_Last',\n",
    "    'Hour_mean': 'Hour_Avg',\n",
    "    'SuspicionLevel_<lambda>': 'NumWithSuspicion',\n",
    "    'LastVerdict_<lambda>': 'NumWithVerdict',\n",
    "    'IncidentGrade_first': 'IncidentGrade',\n",
    "}\n",
    "\n",
    "incident_agg = incident_agg.rename(columns=rename_map)\n",
    "incident_agg = incident_agg.drop(columns=['Timestamp_min', 'Timestamp_max'], errors='ignore')\n",
    "\n",
    "# Separa features e target\n",
    "X_test = incident_agg.drop(columns=['IncidentId', 'IncidentGrade'])\n",
    "y_test = incident_agg['IncidentGrade']\n",
    "\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregazione a livello Incident\n",
    "def get_mode(x):\n",
    "    mode = x.mode()\n",
    "    return mode[0] if len(mode) > 0 else x.iloc[0] if len(x) > 0 else None\n",
    "\n",
    "agg_dict = {\n",
    "    'IncidentGrade': 'first',\n",
    "    'AlertId': 'nunique',\n",
    "    'Id': 'count',\n",
    "    'EntityType': 'nunique',\n",
    "    'EvidenceRole': 'nunique',\n",
    "    'Category': get_mode,\n",
    "    'Hour': ['min', 'max', 'mean'],\n",
    "    'DayOfWeek': get_mode,\n",
    "    'IsWeekend': 'max',\n",
    "    'Timestamp': ['min', 'max'],\n",
    "    'SuspicionLevel': lambda x: x.notna().sum(),\n",
    "    'LastVerdict': lambda x: x.notna().sum(),\n",
    "}\n",
    "\n",
    "# Aggiungi colonne MITRE\n",
    "mitre_cols = [col for col in df_test.columns if col.startswith('MITRE_')]\n",
    "for col in mitre_cols:\n",
    "    agg_dict[col] = 'sum'\n",
    "\n",
    "incident_agg = df_test.groupby('IncidentId').agg(agg_dict).reset_index()\n",
    "\n",
    "# Flatten colonne\n",
    "incident_agg.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col \n",
    "                        for col in incident_agg.columns.values]\n",
    "\n",
    "print(f\"Dataset aggregato: {incident_agg.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features temporali\n",
    "df_test['Timestamp'] = pd.to_datetime(df_test['Timestamp'])\n",
    "df_test['Hour'] = df_test['Timestamp'].dt.hour\n",
    "df_test['DayOfWeek'] = df_test['Timestamp'].dt.dayofweek\n",
    "df_test['IsWeekend'] = (df_test['DayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "print(\"Features temporali create\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding MITRE\n",
    "def encode_mitre(techniques_str, frequent_techs):\n",
    "    techniques = set(techniques_str.split(';'))\n",
    "    features = {f'MITRE_{tech}': 0 for tech in frequent_techs}\n",
    "    features['MITRE_unknown'] = 1 if 'unknown' in techniques else 0\n",
    "    features['MITRE_n_rare'] = 0\n",
    "    \n",
    "    for tech in techniques:\n",
    "        if tech in frequent_techs:\n",
    "            features[f'MITRE_{tech}'] = 1\n",
    "        elif tech != 'unknown':\n",
    "            features['MITRE_n_rare'] += 1\n",
    "    \n",
    "    features['MITRE_n_rare'] = min(features['MITRE_n_rare'], 5)\n",
    "    return features\n",
    "\n",
    "mitre_encoded = pd.DataFrame([\n",
    "    encode_mitre(tech, frequent_techniques) \n",
    "    for tech in df_test['MitreTechniques_normalized']\n",
    "])\n",
    "\n",
    "df_test = pd.concat([df_test, mitre_encoded], axis=1)\n",
    "\n",
    "print(f\"Features MITRE create: {mitre_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizza codici MITRE (stessa funzione del training)\n",
    "def normalize_mitre(technique):\n",
    "    if pd.isna(technique):\n",
    "        return 'unknown'\n",
    "    techniques = str(technique).split(';')\n",
    "    normalized = []\n",
    "    for t in techniques:\n",
    "        t = t.strip()\n",
    "        if not t.startswith('T') and t != 'unknown':\n",
    "            t = 'T' + t\n",
    "        if '.' in t and t != 'unknown':\n",
    "            t = t.split('.')[0]\n",
    "        normalized.append(t)\n",
    "    return ';'.join(sorted(set(normalized)))\n",
    "\n",
    "df_test['MitreTechniques_normalized'] = df_test['MitreTechniques'].apply(normalize_mitre)\n",
    "\n",
    "# Conta tecniche per determinare quali sono frequenti\n",
    "all_techniques = []\n",
    "for techniques in df_test['MitreTechniques_normalized']:\n",
    "    all_techniques.extend(techniques.split(';'))\n",
    "technique_counts = Counter(all_techniques)\n",
    "\n",
    "# Usa stessa soglia del training (0.5%)\n",
    "min_occurrences = len(df_test) * 0.005\n",
    "frequent_techniques = [tech for tech, count in technique_counts.items() \n",
    "                      if count >= min_occurrences and tech != 'unknown']\n",
    "\n",
    "print(f\"Tecniche frequenti selezionate: {len(frequent_techniques)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062468d",
   "metadata": {},
   "source": [
    "## 4. Valutazione Performance su Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path('../models')\n",
    "\n",
    "# Carica modelli e fai predizioni\n",
    "models_results = {}\n",
    "\n",
    "# XGBoost\n",
    "xgb_path = models_dir / 'xgboost' / 'model.pkl'\n",
    "if xgb_path.exists():\n",
    "    with open(xgb_path, 'rb') as f:\n",
    "        xgb_model = pickle.load(f)\n",
    "    with open(models_dir / 'xgboost' / 'label_encoder.pkl', 'rb') as f:\n",
    "        xgb_le = pickle.load(f)\n",
    "    \n",
    "    y_test_xgb = xgb_le.transform(y_test)\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    \n",
    "    models_results['XGBoost'] = {\n",
    "        'y_true': y_test_xgb,\n",
    "        'y_pred': y_pred_xgb,\n",
    "        'label_encoder': xgb_le,\n",
    "        'model': xgb_model\n",
    "    }\n",
    "    print(\"✓ XGBoost caricato e predizioni completate\")\n",
    "\n",
    "# Random Forest\n",
    "rf_path = models_dir / 'random_forest' / 'model.pkl'\n",
    "if rf_path.exists():\n",
    "    with open(rf_path, 'rb') as f:\n",
    "        rf_model = pickle.load(f)\n",
    "    with open(models_dir / 'random_forest' / 'label_encoder.pkl', 'rb') as f:\n",
    "        rf_le = pickle.load(f)\n",
    "    \n",
    "    y_test_rf = rf_le.transform(y_test)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    \n",
    "    models_results['Random Forest'] = {\n",
    "        'y_true': y_test_rf,\n",
    "        'y_pred': y_pred_rf,\n",
    "        'label_encoder': rf_le,\n",
    "        'model': rf_model\n",
    "    }\n",
    "    print(\"✓ Random Forest caricato e predizioni completate\")\n",
    "\n",
    "# K-Means\n",
    "kmeans_path = models_dir / 'kmeans' / 'model.pkl'\n",
    "if kmeans_path.exists():\n",
    "    with open(kmeans_path, 'rb') as f:\n",
    "        kmeans_model = pickle.load(f)\n",
    "    with open(models_dir / 'kmeans' / 'scaler.pkl', 'rb') as f:\n",
    "        kmeans_scaler = pickle.load(f)\n",
    "    \n",
    "    X_test_scaled = kmeans_scaler.transform(X_test)\n",
    "    y_pred_kmeans = kmeans_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Encode ground truth per confronto\n",
    "    kmeans_le = LabelEncoder()\n",
    "    y_test_kmeans = kmeans_le.fit_transform(y_test)\n",
    "    \n",
    "    models_results['K-Means'] = {\n",
    "        'y_true': y_test_kmeans,\n",
    "        'y_pred': y_pred_kmeans,\n",
    "        'label_encoder': kmeans_le,\n",
    "        'model': kmeans_model,\n",
    "        'X_scaled': X_test_scaled\n",
    "    }\n",
    "    print(\"✓ K-Means caricato e predizioni completate\")\n",
    "\n",
    "print(f\"\\nModelli valutati: {list(models_results.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola metriche per modelli supervisionati\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in ['XGBoost', 'Random Forest']:\n",
    "    if model_name in models_results:\n",
    "        result = models_results[model_name]\n",
    "        \n",
    "        accuracy = accuracy_score(result['y_true'], result['y_pred'])\n",
    "        macro_f1 = f1_score(result['y_true'], result['y_pred'], average='macro')\n",
    "        \n",
    "        # F1 per classe\n",
    "        f1_per_class = f1_score(result['y_true'], result['y_pred'], average=None)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Macro F1': macro_f1,\n",
    "            'F1 Class 0': f1_per_class[0],\n",
    "            'F1 Class 1': f1_per_class[1],\n",
    "            'F1 Class 2': f1_per_class[2],\n",
    "            'N Test': len(result['y_true'])\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Test Macro F1', ascending=False)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"CONFRONTO MODELLI SU GUIDE_TEST.CSV\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\n⭐ Best Model: {comparison_df.iloc[0]['Model']}\")\n",
    "print(f\"   Macro F1: {comparison_df.iloc[0]['Test Macro F1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365dbba",
   "metadata": {},
   "source": [
    "## 5. Classification Reports e Confusion Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Reports\n",
    "for model_name in ['XGBoost', 'Random Forest']:\n",
    "    if model_name in models_results:\n",
    "        result = models_results[model_name]\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{model_name} - Classification Report\")\n",
    "        print(\"=\" * 70)\n",
    "        print(classification_report(\n",
    "            result['y_true'], \n",
    "            result['y_pred'],\n",
    "            target_names=result['label_encoder'].classes_,\n",
    "            digits=4\n",
    "        ))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-Score per classe\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "width = 0.35\n",
    "x = np.arange(3)  # 3 classi\n",
    "\n",
    "if 'XGBoost' in models_results and 'Random Forest' in models_results:\n",
    "    xgb_f1 = [comparison_df[comparison_df['Model'] == 'XGBoost']['F1 Class 0'].values[0],\n",
    "              comparison_df[comparison_df['Model'] == 'XGBoost']['F1 Class 1'].values[0],\n",
    "              comparison_df[comparison_df['Model'] == 'XGBoost']['F1 Class 2'].values[0]]\n",
    "    \n",
    "    rf_f1 = [comparison_df[comparison_df['Model'] == 'Random Forest']['F1 Class 0'].values[0],\n",
    "             comparison_df[comparison_df['Model'] == 'Random Forest']['F1 Class 1'].values[0],\n",
    "             comparison_df[comparison_df['Model'] == 'Random Forest']['F1 Class 2'].values[0]]\n",
    "    \n",
    "    ax.bar(x - width/2, xgb_f1, width, label='XGBoost', color='steelblue', edgecolor='black')\n",
    "    ax.bar(x + width/2, rf_f1, width, label='Random Forest', color='forestgreen', edgecolor='black')\n",
    "    \n",
    "    # Ottieni nomi classi dal primo modello\n",
    "    class_names = models_results['XGBoost']['label_encoder'].classes_\n",
    "    \n",
    "    ax.set_ylabel('F1-Score')\n",
    "    ax.set_title('F1-Score per Classe')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee50173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva report di confronto su test set reale\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'test_dataset': 'GUIDE_Test.csv',\n",
    "    'n_test_samples': int(len(y_test)),\n",
    "    'n_features': int(X_test.shape[1]),\n",
    "    'best_model': best_model['Model'],\n",
    "    'best_test_f1': float(best_model['Test Macro F1']),\n",
    "    'best_test_accuracy': float(best_model['Test Accuracy']),\n",
    "    'models_comparison': comparison_df.to_dict(orient='records'),\n",
    "}\n",
    "\n",
    "# Aggiungi ROC AUC scores\n",
    "if roc_results:\n",
    "    report['roc_auc_comparison'] = {}\n",
    "    for model_name, roc_data in roc_results.items():\n",
    "        n_classes = len(roc_data['label_encoder'].classes_)\n",
    "        report['roc_auc_comparison'][model_name] = {\n",
    "            'per_class': {roc_data['label_encoder'].classes_[i]: float(roc_data['roc_auc'][i]) \n",
    "                         for i in range(n_classes)},\n",
    "            'micro_average': float(roc_data['roc_auc']['micro'])\n",
    "        }\n",
    "\n",
    "if len(feature_importance_dict) >= 2:\n",
    "    report['top_features'] = top_features.head(10)['Mean'].to_dict()\n",
    "\n",
    "if 'K-Means' in models_results:\n",
    "    result = models_results['K-Means']\n",
    "    ari = adjusted_rand_score(result['y_true'], result['y_pred'])\n",
    "    nmi = normalized_mutual_info_score(result['y_true'], result['y_pred'])\n",
    "    silhouette = silhouette_score(result['X_scaled'], result['y_pred'])\n",
    "    \n",
    "    report['kmeans_metrics'] = {\n",
    "        'silhouette': float(silhouette),\n",
    "        'ari': float(ari),\n",
    "        'nmi': float(nmi)\n",
    "    }\n",
    "\n",
    "# Aggiungi classification reports dettagliati\n",
    "for model_name in ['XGBoost', 'Random Forest']:\n",
    "    if model_name in models_results:\n",
    "        result = models_results[model_name]\n",
    "        report[f'{model_name.lower().replace(\" \", \"_\")}_report'] = classification_report(\n",
    "            result['y_true'], \n",
    "            result['y_pred'],\n",
    "            target_names=result['label_encoder'].classes_.tolist(),\n",
    "            output_dict=True\n",
    "        )\n",
    "\n",
    "with open('../models/test_comparison_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Salva anche CSV\n",
    "comparison_df.to_csv('../models/test_models_comparison.csv', index=False)\n",
    "\n",
    "# Salva ROC AUC scores in CSV separato\n",
    "if roc_results:\n",
    "    roc_df_data = []\n",
    "    for model_name, roc_data in roc_results.items():\n",
    "        n_classes = len(roc_data['label_encoder'].classes_)\n",
    "        for i in range(n_classes):\n",
    "            class_name = roc_data['label_encoder'].classes_[i]\n",
    "            roc_df_data.append({\n",
    "                'Model': model_name,\n",
    "                'Class': class_name,\n",
    "                'AUC': roc_data['roc_auc'][i]\n",
    "            })\n",
    "        roc_df_data.append({\n",
    "            'Model': model_name,\n",
    "            'Class': 'Micro-average',\n",
    "            'AUC': roc_data['roc_auc']['micro']\n",
    "        })\n",
    "    \n",
    "    roc_df = pd.DataFrame(roc_df_data)\n",
    "    roc_df.to_csv('../models/test_roc_auc_scores.csv', index=False)\n",
    "    print(\"ROC AUC scores salvati in: ../models/test_roc_auc_scores.csv\")\n",
    "\n",
    "print(\"Report salvati:\")\n",
    "print(\"  - ../models/test_comparison_report.json\")\n",
    "print(\"  - ../models/test_models_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f69ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Micro-average ROC comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors_models = {'XGBoost': 'blue', 'Random Forest': 'green'}\n",
    "\n",
    "for model_name, roc_data in roc_results.items():\n",
    "    color = colors_models.get(model_name, 'black')\n",
    "    plt.plot(roc_data['fpr']['micro'], roc_data['tpr']['micro'],\n",
    "            color=color, lw=2,\n",
    "            label=f\"{model_name} (AUC={roc_data['roc_auc']['micro']:.4f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1, label='Random Classifier')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison - Micro-average (All Classes)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbaf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves per classe (side-by-side per modelli)\n",
    "if len(roc_results) >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    colors_models = {'XGBoost': 'blue', 'Random Forest': 'green'}\n",
    "    \n",
    "    # Ottieni nomi classi dal primo modello\n",
    "    first_model = list(roc_results.keys())[0]\n",
    "    class_names = roc_results[first_model]['label_encoder'].classes_\n",
    "    \n",
    "    # Plot per ogni classe\n",
    "    for class_idx in range(3):\n",
    "        ax = axes[class_idx]\n",
    "        \n",
    "        for model_name, roc_data in roc_results.items():\n",
    "            color = colors_models.get(model_name, 'black')\n",
    "            ax.plot(roc_data['fpr'][class_idx], roc_data['tpr'][class_idx],\n",
    "                   color=color, lw=2,\n",
    "                   label=f\"{model_name} (AUC={roc_data['roc_auc'][class_idx]:.4f})\")\n",
    "        \n",
    "        # Diagonal line\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(f'ROC Curve - {class_names[class_idx]}')\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Calcola ROC curves per tutti i modelli supervisionati\n",
    "roc_results = {}\n",
    "\n",
    "for model_name in ['XGBoost', 'Random Forest']:\n",
    "    if model_name in models_results:\n",
    "        result = models_results[model_name]\n",
    "        \n",
    "        # Ottieni probabilità\n",
    "        y_pred_proba = result['model'].predict_proba(X_test)\n",
    "        \n",
    "        # Binarizza labels\n",
    "        y_test_bin = label_binarize(result['y_true'], classes=[0, 1, 2])\n",
    "        n_classes = y_test_bin.shape[1]\n",
    "        \n",
    "        # Calcola ROC e AUC per ogni classe\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Micro-average\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        \n",
    "        roc_results[model_name] = {\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'roc_auc': roc_auc,\n",
    "            'label_encoder': result['label_encoder']\n",
    "        }\n",
    "\n",
    "print(\"ROC AUC Scores per modello:\")\n",
    "for model_name, roc_data in roc_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for i, class_name in enumerate(roc_data['label_encoder'].classes_):\n",
    "        print(f\"  {class_name}: {roc_data['roc_auc'][i]:.4f}\")\n",
    "    print(f\"  Micro-average: {roc_data['roc_auc']['micro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75167f69",
   "metadata": {},
   "source": [
    "## 6.1 ROC Curves Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confronto metriche\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = comparison_df['Model'].values\n",
    "x = np.arange(len(models))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(x, comparison_df['Test Accuracy'], color='skyblue', edgecolor='black')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Test Accuracy su GUIDE_Test.csv')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['Test Accuracy']):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Macro F1\n",
    "axes[1].bar(x, comparison_df['Test Macro F1'], color='coral', edgecolor='black')\n",
    "axes[1].set_ylabel('Macro F1-Score')\n",
    "axes[1].set_title('Test Macro F1-Score su GUIDE_Test.csv')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comparison_df['Test Macro F1']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77e1fd",
   "metadata": {},
   "source": [
    "## 6. Visualizzazione Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ab52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, model_name in enumerate(['XGBoost', 'Random Forest']):\n",
    "    if model_name in models_results:\n",
    "        result = models_results[model_name]\n",
    "        \n",
    "        cm = confusion_matrix(result['y_true'], result['y_pred'])\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2%', \n",
    "                   cmap='Blues' if idx == 0 else 'Greens',\n",
    "                   xticklabels=result['label_encoder'].classes_,\n",
    "                   yticklabels=result['label_encoder'].classes_,\n",
    "                   ax=axes[idx])\n",
    "        axes[idx].set_title(f'{model_name} - Confusion Matrix')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab8458",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica feature importance\n",
    "feature_importance_dict = {}\n",
    "\n",
    "for model_name in supervised_models.keys():\n",
    "    fi_file = models_dir / model_name / 'feature_importance.csv'\n",
    "    if fi_file.exists():\n",
    "        fi_df = pd.read_csv(fi_file)\n",
    "        feature_importance_dict[model_name] = fi_df.set_index('Feature')['Importance']\n",
    "\n",
    "print(f\"Feature importance caricata per: {list(feature_importance_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 features comuni\n",
    "if len(feature_importance_dict) >= 2:\n",
    "    # Combina importance da tutti i modelli\n",
    "    combined_fi = pd.DataFrame(feature_importance_dict)\n",
    "    \n",
    "    # Media delle importance (normalizzate)\n",
    "    combined_fi_norm = combined_fi.div(combined_fi.sum(axis=0), axis=1)\n",
    "    combined_fi_norm['Mean'] = combined_fi_norm.mean(axis=1)\n",
    "    top_features = combined_fi_norm.nlargest(15, 'Mean')\n",
    "    \n",
    "    print(\"Top 15 Features (media normalizzata):\")\n",
    "    print(top_features[['Mean']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314cd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-Score per classe\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "width = 0.35\n",
    "\n",
    "if 'XGBoost' in models_results and 'Random Forest' in models_results:\n",
    "    # Numero di classi dinamico\n",
    "    n_classes = len(models_results['XGBoost']['label_encoder'].classes_)\n",
    "    x = np.arange(n_classes)\n",
    "    \n",
    "    xgb_f1 = [comparison_df[comparison_df['Model'] == 'XGBoost'][f'F1 Class {i}'].values[0] \n",
    "              for i in range(n_classes)]\n",
    "    \n",
    "    rf_f1 = [comparison_df[comparison_df['Model'] == 'Random Forest'][f'F1 Class {i}'].values[0]\n",
    "             for i in range(n_classes)]\n",
    "    \n",
    "    ax.bar(x - width/2, xgb_f1, width, label='XGBoost', color='steelblue', edgecolor='black')\n",
    "    ax.bar(x + width/2, rf_f1, width, label='Random Forest', color='forestgreen', edgecolor='black')\n",
    "    \n",
    "    # Ottieni nomi classi\n",
    "    class_names = models_results['XGBoost']['label_encoder'].classes_\n",
    "    \n",
    "    ax.set_ylabel('F1-Score')\n",
    "    ax.set_title('F1-Score per Classe')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247cfd4",
   "metadata": {},
   "source": [
    "## 8. K-Means Analysis su Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'K-Means' in models_results:\n",
    "    result = models_results['K-Means']\n",
    "    \n",
    "    # Calcola metriche\n",
    "    silhouette = silhouette_score(result['X_scaled'], result['y_pred'])\n",
    "    ari = adjusted_rand_score(result['y_true'], result['y_pred'])\n",
    "    nmi = normalized_mutual_info_score(result['y_true'], result['y_pred'])\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"K-MEANS CLUSTERING SU GUIDE_TEST.CSV\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Numero cluster: {result['model'].n_clusters}\")\n",
    "    print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"Inertia: {result['model'].inertia_:.2f}\")\n",
    "    print(f\"\\nConfronto con IncidentGrade (ground truth):\")\n",
    "    print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
    "    print(f\"  Normalized Mutual Info: {nmi:.4f}\")\n",
    "    \n",
    "    # Distribuzione cluster\n",
    "    unique, counts = np.unique(result['y_pred'], return_counts=True)\n",
    "    print(f\"\\nDistribuzione cluster:\")\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        pct = count / len(result['y_pred']) * 100\n",
    "        print(f\"  Cluster {cluster_id}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Crosstab cluster vs IncidentGrade\n",
    "    cluster_vs_grade = pd.crosstab(\n",
    "        result['y_pred'], y_test,\n",
    "        rownames=['Cluster'],\n",
    "        colnames=['IncidentGrade']\n",
    "    )\n",
    "    cluster_vs_grade_norm = cluster_vs_grade.div(cluster_vs_grade.sum(axis=1), axis=0)\n",
    "    \n",
    "    print(\"\\nDistribuzione IncidentGrade per Cluster:\")\n",
    "    print(cluster_vs_grade_norm.round(3))\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(cluster_vs_grade_norm, annot=True, fmt='.2%', cmap='YlOrRd')\n",
    "    plt.title('Distribuzione IncidentGrade per Cluster (GUIDE_Test)')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.xlabel('IncidentGrade')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060dcd8",
   "metadata": {},
   "source": [
    "## 9. Riepilogo e Raccomandazioni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99071268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves per classe (side-by-side per modelli)\n",
    "if len(roc_results) >= 2:\n",
    "    # Ottieni nomi classi e numero dal primo modello\n",
    "    first_model = list(roc_results.keys())[0]\n",
    "    class_names = roc_results[first_model]['label_encoder'].classes_\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_classes, figsize=(6 * n_classes, 5))\n",
    "    \n",
    "    # Se c'è solo 1 classe, axes non è un array\n",
    "    if n_classes == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors_models = {'XGBoost': 'blue', 'Random Forest': 'green'}\n",
    "    \n",
    "    # Plot per ogni classe\n",
    "    for class_idx in range(n_classes):\n",
    "        ax = axes[class_idx]\n",
    "        \n",
    "        for model_name, roc_data in roc_results.items():\n",
    "            color = colors_models.get(model_name, 'black')\n",
    "            ax.plot(roc_data['fpr'][class_idx], roc_data['tpr'][class_idx],\n",
    "                   color=color, lw=2,\n",
    "                   label=f\"{model_name} (AUC={roc_data['roc_auc'][class_idx]:.4f})\")\n",
    "        \n",
    "        # Diagonal line\n",
    "        ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(f'ROC Curve - {class_names[class_idx]}')\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7b5f4",
   "metadata": {},
   "source": [
    "## 10. Salva Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7104ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot AUC comparison\n",
    "if len(roc_results) >= 2:\n",
    "    models_list = list(roc_results.keys())\n",
    "    class_names = roc_results[models_list[0]]['label_encoder'].classes_\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # AUC per classe\n",
    "    x = np.arange(n_classes)\n",
    "    width = 0.35\n",
    "    \n",
    "    for idx, model_name in enumerate(models_list):\n",
    "        auc_scores = [roc_results[model_name]['roc_auc'][i] for i in range(n_classes)]\n",
    "        offset = width * (idx - 0.5)\n",
    "        color = 'steelblue' if idx == 0 else 'forestgreen'\n",
    "        axes[0].bar(x + offset, auc_scores, width, label=model_name, \n",
    "                   color=color, edgecolor='black')\n",
    "    \n",
    "    axes[0].set_ylabel('AUC Score')\n",
    "    axes[0].set_title('ROC AUC per Classe')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(class_names)\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Micro-average AUC\n",
    "    micro_aucs = [roc_results[model]['roc_auc']['micro'] for model in models_list]\n",
    "    colors_bar = ['steelblue', 'forestgreen']\n",
    "    axes[1].bar(range(len(models_list)), micro_aucs, color=colors_bar, edgecolor='black')\n",
    "    axes[1].set_ylabel('Micro-average AUC')\n",
    "    axes[1].set_title('ROC AUC Micro-average')\n",
    "    axes[1].set_xticks(range(len(models_list)))\n",
    "    axes[1].set_xticklabels(models_list)\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(micro_aucs):\n",
    "        axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
