{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - GUIDE Dataset\n",
    "\n",
    "**Obiettivo:** Preparare il dataset GUIDE per il machine learning, creando features significative a livello Incident.\n",
    "\n",
    "**Task:** Classificazione multi-classe di IncidentGrade (TruePositive, BenignPositive, FalsePositive)\n",
    "\n",
    "**Approccio:**\n",
    "1. Aggregare dati da Evidence → Alert → Incident level\n",
    "2. Creare features numeriche e categoriche\n",
    "3. Gestire valori mancanti\n",
    "4. Codificare variabili categoriche\n",
    "5. Salvare dataset processato per modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Caricamento Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerie importate con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset pulito dall'EDA\n",
    "file_path = '../data/GUIDE_Train.csv'\n",
    "\n",
    "print(\"Caricamento dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Dataset caricato: {df.shape[0]:,} righe, {df.shape[1]} colonne\")\n",
    "print(f\"Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pulizia Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rimuovi record senza target\n",
    "print(f\"Record con IncidentGrade nullo: {df['IncidentGrade'].isna().sum()}\")\n",
    "df = df[df['IncidentGrade'].notna()].copy()\n",
    "\n",
    "# Rimuovi colonne con >97% missing\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "cols_to_drop = missing_pct[missing_pct > 97].index.tolist()\n",
    "print(f\"\\nColonne rimosse (>97% missing): {cols_to_drop}\")\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Rimuovi duplicati su Id\n",
    "duplicati = df['Id'].duplicated().sum()\n",
    "print(f\"\\nDuplicati su Id: {duplicati}\")\n",
    "if duplicati > 0:\n",
    "    df = df.drop_duplicates(subset=['Id'], keep='first')\n",
    "\n",
    "print(f\"\\nDimensioni dopo pulizia: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parsing Temporale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti Timestamp\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
    "df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "df['TimeOfDay'] = pd.cut(df['Hour'], bins=[0, 6, 12, 18, 24], \n",
    "                         labels=['Night', 'Morning', 'Afternoon', 'Evening'], \n",
    "                         include_lowest=True)\n",
    "\n",
    "print(\"Features temporali create\")\n",
    "print(f\"Range temporale: {df['Timestamp'].min()} - {df['Timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregazione a Livello Incident\n",
    "\n",
    "Questo è il passo più importante: trasformiamo le evidenze in features a livello incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creazione features aggregate per incident...\\n\")\n",
    "\n",
    "# Funzione per ottenere il valore più comune (moda)\n",
    "def get_mode(x):\n",
    "    mode = x.mode()\n",
    "    return mode[0] if len(mode) > 0 else x.iloc[0] if len(x) > 0 else None\n",
    "\n",
    "# Aggregazioni per IncidentId\n",
    "incident_agg = df.groupby('IncidentId').agg({\n",
    "    # Target (sempre uguale per lo stesso incident)\n",
    "    'IncidentGrade': 'first',\n",
    "    \n",
    "    # Conteggi strutturali\n",
    "    'AlertId': 'nunique',           # Numero di alert nell'incident\n",
    "    'Id': 'count',                   # Numero totale di evidenze\n",
    "    'EntityType': 'nunique',         # Diversità di entity types\n",
    "    'EvidenceRole': 'nunique',       # Diversità di evidence roles\n",
    "    \n",
    "    # Features categoriche (moda)\n",
    "    'Category': get_mode,\n",
    "    'DetectorId': get_mode,\n",
    "    'OrgId': 'first',\n",
    "    \n",
    "    # Features temporali\n",
    "    'Hour': ['min', 'max', 'mean'],\n",
    "    'DayOfWeek': get_mode,\n",
    "    'IsWeekend': 'max',\n",
    "    'Timestamp': ['min', 'max'],     # Per calcolare durata\n",
    "    \n",
    "    # Features geografiche\n",
    "    'CountryCode': 'nunique',\n",
    "    'State': 'nunique',\n",
    "    'City': 'nunique',\n",
    "    \n",
    "    # Features tecniche\n",
    "    'DeviceId': 'nunique',\n",
    "    'OSFamily': 'nunique',\n",
    "    'OSVersion': 'nunique',\n",
    "    \n",
    "    # Features di sicurezza\n",
    "    'SuspicionLevel': lambda x: x.notna().sum(),  # Quante evidenze hanno suspicion\n",
    "    'LastVerdict': lambda x: x.notna().sum(),     # Quante hanno verdict\n",
    "}).reset_index()\n",
    "\n",
    "# Rinomina colonne multi-livello\n",
    "incident_agg.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col \n",
    "                        for col in incident_agg.columns.values]\n",
    "\n",
    "print(f\"Dataset aggregato creato: {incident_agg.shape}\")\n",
    "print(f\"\\nPrime colonne: {list(incident_agg.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola durata incident (differenza tra prima e ultima evidenza)\n",
    "incident_agg['Duration_seconds'] = (\n",
    "    pd.to_datetime(incident_agg['Timestamp_max']) - \n",
    "    pd.to_datetime(incident_agg['Timestamp_min'])\n",
    ").dt.total_seconds()\n",
    "\n",
    "# Rinomina colonne per chiarezza\n",
    "rename_map = {\n",
    "    'AlertId_nunique': 'NumAlerts',\n",
    "    'Id_count': 'NumEvidences',\n",
    "    'EntityType_nunique': 'NumEntityTypes',\n",
    "    'EvidenceRole_nunique': 'NumEvidenceRoles',\n",
    "    'Hour_min': 'Hour_First',\n",
    "    'Hour_max': 'Hour_Last',\n",
    "    'Hour_mean': 'Hour_Avg',\n",
    "    'CountryCode_nunique': 'NumCountries',\n",
    "    'State_nunique': 'NumStates',\n",
    "    'City_nunique': 'NumCities',\n",
    "    'DeviceId_nunique': 'NumDevices',\n",
    "    'OSFamily_nunique': 'NumOSFamilies',\n",
    "    'OSVersion_nunique': 'NumOSVersions',\n",
    "    'SuspicionLevel_<lambda>': 'NumWithSuspicion',\n",
    "    'LastVerdict_<lambda>': 'NumWithVerdict',\n",
    "}\n",
    "\n",
    "incident_agg = incident_agg.rename(columns=rename_map)\n",
    "\n",
    "# Rimuovi colonne timestamp originali\n",
    "incident_agg = incident_agg.drop(columns=['Timestamp_min', 'Timestamp_max'], errors='ignore')\n",
    "\n",
    "print(f\"Features ingegnerizzate: {incident_agg.shape[1] - 2}\")  # -2 per IncidentId e target\n",
    "incident_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Avanzato - MITRE Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizza MITRE Techniques\n",
    "print(\"Processing MITRE Techniques...\")\n",
    "\n",
    "# Crea features da MitreTechniques\n",
    "mitre_features = df.groupby('IncidentId')['MitreTechniques'].agg([\n",
    "    ('NumWithMitre', lambda x: x.notna().sum()),  # Quante evidenze hanno tecniche MITRE\n",
    "    ('NumUniqueMitre', lambda x: len(set(','.join(x.dropna().astype(str)).split(',')))),  # Tecniche uniche\n",
    "]).reset_index()\n",
    "\n",
    "# Merge con dataset principale\n",
    "incident_agg = incident_agg.merge(mitre_features, on='IncidentId', how='left')\n",
    "\n",
    "print(f\"Features MITRE aggiunte. Shape: {incident_agg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gestione Valori Categorici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica colonne categoriche\n",
    "categorical_cols = incident_agg.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Rimuovi IncidentId e IncidentGrade\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['IncidentId', 'IncidentGrade']]\n",
    "\n",
    "print(f\"Colonne categoriche da processare: {categorical_cols}\")\n",
    "\n",
    "# Per colonne con alta cardinalità, sostituisci valori rari con 'Other'\n",
    "for col in categorical_cols:\n",
    "    if incident_agg[col].nunique() > 100:\n",
    "        # Mantieni solo i top 50 valori\n",
    "        top_values = incident_agg[col].value_counts().head(50).index\n",
    "        incident_agg[col] = incident_agg[col].apply(lambda x: x if x in top_values else 'Other')\n",
    "        print(f\"  {col}: ridotto a {incident_agg[col].nunique()} categorie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preparazione per ML: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa features e target\n",
    "X = incident_agg.drop(columns=['IncidentId', 'IncidentGrade'])\n",
    "y = incident_agg['IncidentGrade']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nDistribuzione target:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nProporzioni:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding per variabili categoriche\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "X_encoded = X.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"Encoded {len(label_encoders)} categorical features\")\n",
    "print(f\"\\nTipi di dati finali:\")\n",
    "print(X_encoded.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestisci missing values\n",
    "print(\"\\nMissing values prima dell'imputazione:\")\n",
    "missing = X_encoded.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Sostituisci NaN con -999 (XGBoost gestisce bene questo approccio)\n",
    "X_encoded = X_encoded.fillna(-999)\n",
    "\n",
    "print(f\"\\nMissing values dopo imputazione: {X_encoded.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split stratificato\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/Test Split completato\")\n",
    "print(f\"\\nX_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"\\nDistribuzione y_train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nDistribuzione y_test:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Salvataggio Dataset Processato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva i dataset processati\n",
    "import pickle\n",
    "\n",
    "# Crea directory per i dati processati\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Salva train/test\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False, header=['IncidentGrade'])\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False, header=['IncidentGrade'])\n",
    "\n",
    "# Salva label encoders\n",
    "with open('../data/processed/label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# Salva anche il dataset aggregato completo\n",
    "incident_agg.to_csv('../data/processed/incident_features.csv', index=False)\n",
    "\n",
    "print(\"Dataset salvati in ../data/processed/\")\n",
    "print(f\"  - X_train.csv: {X_train.shape}\")\n",
    "print(f\"  - X_test.csv: {X_test.shape}\")\n",
    "print(f\"  - y_train.csv: {y_train.shape}\")\n",
    "print(f\"  - y_test.csv: {y_test.shape}\")\n",
    "print(f\"  - incident_features.csv: {incident_agg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analisi Features Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista features create\n",
    "print(f\"Totale features: {X_encoded.shape[1]}\")\n",
    "print(f\"\\nLista completa features:\")\n",
    "for i, col in enumerate(X_encoded.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiche descrittive delle features numeriche principali\n",
    "key_features = ['NumAlerts', 'NumEvidences', 'NumEntityTypes', 'NumDevices', \n",
    "                'Duration_seconds', 'NumWithMitre', 'NumCountries']\n",
    "\n",
    "available_features = [f for f in key_features if f in X_encoded.columns]\n",
    "X_encoded[available_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza distribuzione di alcune features chiave per target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_features = ['NumAlerts', 'NumEvidences', 'NumEntityTypes', \n",
    "                 'NumDevices', 'Duration_seconds', 'NumWithMitre']\n",
    "\n",
    "for i, feature in enumerate(plot_features):\n",
    "    if feature in incident_agg.columns:\n",
    "        for grade in incident_agg['IncidentGrade'].unique():\n",
    "            data = incident_agg[incident_agg['IncidentGrade'] == grade][feature]\n",
    "            axes[i].hist(data, alpha=0.5, label=grade, bins=30)\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequenza')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_title(f'Distribuzione {feature} per IncidentGrade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Riepilogo Feature Engineering\n",
    "\n",
    "**Features create:**\n",
    "1. **Conteggi strutturali:** NumAlerts, NumEvidences, NumEntityTypes, NumEvidenceRoles\n",
    "2. **Features temporali:** Hour_First, Hour_Last, Hour_Avg, DayOfWeek, IsWeekend, Duration_seconds\n",
    "3. **Features geografiche:** NumCountries, NumStates, NumCities\n",
    "4. **Features tecniche:** NumDevices, NumOSFamilies, NumOSVersions\n",
    "5. **Features di sicurezza:** NumWithSuspicion, NumWithVerdict, NumWithMitre, NumUniqueMitre\n",
    "6. **Features categoriche:** Category, DetectorId, OrgId (encoded)\n",
    "\n",
    "**Prossimi passi:**\n",
    "- Training modelli (XGBoost, LightGBM, Random Forest)\n",
    "- Ottimizzazione iperparametri\n",
    "- Feature importance analysis\n",
    "- Cross-validation con macro-F1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}