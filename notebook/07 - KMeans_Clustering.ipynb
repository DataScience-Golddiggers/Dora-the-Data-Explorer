{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e601a46",
   "metadata": {},
   "source": [
    "# Clustering Analysis - K-Means\n",
    "\n",
    "**Obiettivo:** Analisi non supervisionata per scoprire pattern negli incidenti di sicurezza\n",
    "\n",
    "**Approccio:**\n",
    "1. Caricamento dati\n",
    "2. Determinazione numero ottimale di cluster (Elbow + Silhouette)\n",
    "3. Training K-Means\n",
    "4. Analisi dei cluster\n",
    "5. Confronto con IncidentGrade\n",
    "6. Salvataggio modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a717e4",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d652441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:22:15.098287Z",
     "start_time": "2025-10-29T12:22:14.725456Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Librerie importate con successo!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie importate con successo!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "8dfdc359",
   "metadata": {},
   "source": [
    "## 2. Caricamento e Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee9274f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:22:17.841471Z",
     "start_time": "2025-10-29T12:22:17.684841Z"
    }
   },
   "source": [
    "print(\"Caricamento dataset...\")\n",
    "\n",
    "# Usa solo il training set per clustering\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv')['IncidentGrade']\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"\\nDistribuzione IncidentGrade:\\n{y_train.value_counts(normalize=True)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset...\n",
      "X_train: (314230, 20)\n",
      "\n",
      "Distribuzione IncidentGrade:\n",
      "IncidentGrade\n",
      "BenignPositive    0.485921\n",
      "FalsePositive     0.301088\n",
      "TruePositive      0.212990\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b70c8268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:22:20.156882Z",
     "start_time": "2025-10-29T12:22:20.115043Z"
    }
   },
   "source": [
    "# Standardizza le features (importante per K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "print(f\"Features standardizzate: {X_scaled.shape}\")\n",
    "print(f\"Media features: {X_scaled.mean():.2e}\")\n",
    "print(f\"Std features: {X_scaled.std():.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features standardizzate: (314230, 20)\n",
      "Media features: -6.25e-18\n",
      "Std features: 1.00\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "9a58b7f7",
   "metadata": {},
   "source": [
    "## 3. Determinazione Numero Ottimale di Cluster"
   ]
  },
  {
   "cell_type": "code",
   "id": "a5d3b731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:25:24.855427Z",
     "start_time": "2025-10-29T12:22:24.250541Z"
    }
   },
   "source": [
    "# Elbow method + Silhouette score\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"Calcolo metriche per diversi valori di k...\\n\")\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    print(f\"k={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo metriche per diversi valori di k...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m     10\u001B[39m kmeans.fit(X_scaled)\n\u001B[32m     11\u001B[39m inertias.append(kmeans.inertia_)\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m silhouette = \u001B[43msilhouette_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkmeans\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlabels_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m silhouette_scores.append(silhouette)\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mk=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: Inertia=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkmeans.inertia_\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Silhouette=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msilhouette\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/utils/_param_validation.py:218\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    214\u001B[39m         skip_parameter_validation=(\n\u001B[32m    215\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    216\u001B[39m         )\n\u001B[32m    217\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    224\u001B[39m     msg = re.sub(\n\u001B[32m    225\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    226\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    227\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    228\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/metrics/cluster/_unsupervised.py:138\u001B[39m, in \u001B[36msilhouette_score\u001B[39m\u001B[34m(X, labels, metric, sample_size, random_state, **kwds)\u001B[39m\n\u001B[32m    136\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    137\u001B[39m         X, labels = X[indices], labels[indices]\n\u001B[32m--> \u001B[39m\u001B[32m138\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(np.mean(\u001B[43msilhouette_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/utils/_param_validation.py:191\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    189\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    190\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    193\u001B[39m func_sig = signature(func)\n\u001B[32m    195\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/metrics/cluster/_unsupervised.py:302\u001B[39m, in \u001B[36msilhouette_samples\u001B[39m\u001B[34m(X, labels, metric, **kwds)\u001B[39m\n\u001B[32m    298\u001B[39m kwds[\u001B[33m\"\u001B[39m\u001B[33mmetric\u001B[39m\u001B[33m\"\u001B[39m] = metric\n\u001B[32m    299\u001B[39m reduce_func = functools.partial(\n\u001B[32m    300\u001B[39m     _silhouette_reduce, labels=labels, label_freqs=label_freqs\n\u001B[32m    301\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m302\u001B[39m results = \u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpairwise_distances_chunked\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_func\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreduce_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    303\u001B[39m intra_clust_dists, inter_clust_dists = results\n\u001B[32m    304\u001B[39m intra_clust_dists = np.concatenate(intra_clust_dists)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/metrics/pairwise.py:2240\u001B[39m, in \u001B[36mpairwise_distances_chunked\u001B[39m\u001B[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001B[39m\n\u001B[32m   2238\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2239\u001B[39m     X_chunk = X[sl]\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m D_chunk = \u001B[43mpairwise_distances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2241\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (X \u001B[38;5;129;01mis\u001B[39;00m Y \u001B[38;5;129;01mor\u001B[39;00m Y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001B[32m   2242\u001B[39m     metric, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2243\u001B[39m ) \u001B[38;5;129;01mis\u001B[39;00m euclidean_distances:\n\u001B[32m   2244\u001B[39m     \u001B[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001B[39;00m\n\u001B[32m   2245\u001B[39m     \u001B[38;5;66;03m# i.e. \"l2\"\u001B[39;00m\n\u001B[32m   2246\u001B[39m     D_chunk.flat[sl.start :: _num_samples(X) + \u001B[32m1\u001B[39m] = \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/utils/_param_validation.py:191\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    189\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    190\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    193\u001B[39m func_sig = signature(func)\n\u001B[32m    195\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/metrics/pairwise.py:2476\u001B[39m, in \u001B[36mpairwise_distances\u001B[39m\u001B[34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001B[39m\n\u001B[32m   2473\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m distance.squareform(distance.pdist(X, metric=metric, **kwds))\n\u001B[32m   2474\u001B[39m     func = partial(distance.cdist, metric=metric, **kwds)\n\u001B[32m-> \u001B[39m\u001B[32m2476\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_parallel_pairwise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/metrics/pairwise.py:1960\u001B[39m, in \u001B[36m_parallel_pairwise\u001B[39m\u001B[34m(X, Y, func, n_jobs, **kwds)\u001B[39m\n\u001B[32m   1957\u001B[39m X, Y, dtype = _return_float_dtype(X, Y)\n\u001B[32m   1959\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m effective_n_jobs(n_jobs) == \u001B[32m1\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1960\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1962\u001B[39m \u001B[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001B[39;00m\n\u001B[32m   1963\u001B[39m fd = delayed(_dist_wrapper)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/utils/_param_validation.py:191\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    189\u001B[39m global_skip_validation = get_config()[\u001B[33m\"\u001B[39m\u001B[33mskip_parameter_validation\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    190\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    193\u001B[39m func_sig = signature(func)\n\u001B[32m    195\u001B[39m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/metrics/pairwise.py:388\u001B[39m, in \u001B[36meuclidean_distances\u001B[39m\u001B[34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001B[39m\n\u001B[32m    382\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m Y_norm_squared.shape != (\u001B[32m1\u001B[39m, Y.shape[\u001B[32m0\u001B[39m]):\n\u001B[32m    383\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    384\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIncompatible dimensions for Y of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mY.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m and \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    385\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mY_norm_squared of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moriginal_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    386\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_euclidean_distances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_norm_squared\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_norm_squared\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msquared\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/metrics/pairwise.py:428\u001B[39m, in \u001B[36m_euclidean_distances\u001B[39m\u001B[34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001B[39m\n\u001B[32m    425\u001B[39m     distances += XX\n\u001B[32m    426\u001B[39m     distances += YY\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m xp_zero = \u001B[43mxp\u001B[49m\u001B[43m.\u001B[49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdistances\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    429\u001B[39m distances = _modify_in_place_if_numpy(\n\u001B[32m    430\u001B[39m     xp, xp.maximum, distances, xp_zero, out=distances\n\u001B[32m    431\u001B[39m )\n\u001B[32m    433\u001B[39m \u001B[38;5;66;03m# Ensure that distances between vectors and themselves are set to 0.0.\u001B[39;00m\n\u001B[32m    434\u001B[39m \u001B[38;5;66;03m# This may not be the case due to floating point rounding errors.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Università/UnivPM/Data Science/Tesina 1/Dora-the-Data-Explorer/.venv/lib/python3.14/site-packages/sklearn/externals/array_api_compat/numpy/_aliases.py:89\u001B[39m, in \u001B[36masarray\u001B[39m\u001B[34m(obj, dtype, device, copy, **kwargs)\u001B[39m\n\u001B[32m     82\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[38;5;66;03m# asarray also adds the copy keyword, which is not present in numpy 1.0.\u001B[39;00m\n\u001B[32m     86\u001B[39m \u001B[38;5;66;03m# asarray() is different enough between numpy, cupy, and dask, the logic\u001B[39;00m\n\u001B[32m     87\u001B[39m \u001B[38;5;66;03m# complicated enough that it's easier to define it separately for each module\u001B[39;00m\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# rather than trying to combine everything into one function in common/\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34masarray\u001B[39m(\n\u001B[32m     90\u001B[39m     obj: Array | \u001B[38;5;28mcomplex\u001B[39m | NestedSequence[\u001B[38;5;28mcomplex\u001B[39m] | SupportsBufferProtocol,\n\u001B[32m     91\u001B[39m     /,\n\u001B[32m     92\u001B[39m     *,\n\u001B[32m     93\u001B[39m     dtype: DType | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m     94\u001B[39m     device: Device | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m     95\u001B[39m     copy: _Copy | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m     96\u001B[39m     **kwargs: Any,\n\u001B[32m     97\u001B[39m ) -> Array:\n\u001B[32m     98\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     99\u001B[39m \u001B[33;03m    Array API compatibility wrapper for asarray().\u001B[39;00m\n\u001B[32m    100\u001B[39m \n\u001B[32m    101\u001B[39m \u001B[33;03m    See the corresponding documentation in the array library and/or the array API\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[33;03m    specification for more details.\u001B[39;00m\n\u001B[32m    103\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    104\u001B[39m     _helpers._check_device(np, device)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0746969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Elbow + Silhouette\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Numero di Cluster (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Numero di Cluster (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score per k')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nMiglior k secondo Silhouette Score: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243e40e",
   "metadata": {},
   "source": [
    "## 4. Training K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa k=3 per confronto con le 3 classi di IncidentGrade\n",
    "n_clusters = 3\n",
    "\n",
    "print(f\"Training K-Means con k={n_clusters}...\\n\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"Training completato!\")\n",
    "print(f\"\\nDistribuzione cluster:\")\n",
    "print(pd.Series(cluster_labels).value_counts().sort_index())\n",
    "print(f\"\\nInertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24494d",
   "metadata": {},
   "source": [
    "## 5. Visualizzazione Cluster (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA per visualizzazione 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Varianza spiegata dalle prime 2 componenti: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cluster in spazio PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cluster K-Means\n",
    "scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                         c=cluster_labels, cmap='viridis', \n",
    "                         alpha=0.5, s=10)\n",
    "axes[0].scatter(pca.transform(kmeans.cluster_centers_)[:, 0],\n",
    "               pca.transform(kmeans.cluster_centers_)[:, 1],\n",
    "               c='red', marker='X', s=200, edgecolors='black', linewidths=2,\n",
    "               label='Centroidi')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].set_title('K-Means Clustering (PCA)')\n",
    "axes[0].legend()\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "\n",
    "# IncidentGrade reale\n",
    "grade_mapping = {grade: i for i, grade in enumerate(y_train.unique())}\n",
    "grade_numeric = y_train.map(grade_mapping)\n",
    "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                          c=grade_numeric, cmap='coolwarm',\n",
    "                          alpha=0.5, s=10)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('IncidentGrade Reale (PCA)')\n",
    "cbar = plt.colorbar(scatter2, ax=axes[1])\n",
    "cbar.set_ticks(range(len(grade_mapping)))\n",
    "cbar.set_ticklabels(grade_mapping.keys())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59dffb9",
   "metadata": {},
   "source": [
    "## 6. Confronto Cluster vs IncidentGrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e361dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab cluster vs IncidentGrade\n",
    "cluster_vs_grade = pd.crosstab(\n",
    "    cluster_labels, y_train,\n",
    "    rownames=['Cluster'],\n",
    "    colnames=['IncidentGrade']\n",
    ")\n",
    "\n",
    "print(\"Cluster vs IncidentGrade (conteggi):\")\n",
    "print(cluster_vs_grade)\n",
    "\n",
    "# Normalizzato per riga\n",
    "cluster_vs_grade_norm = cluster_vs_grade.div(cluster_vs_grade.sum(axis=1), axis=0)\n",
    "print(\"\\nCluster vs IncidentGrade (percentuali per cluster):\")\n",
    "print(cluster_vs_grade_norm.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cluster_vs_grade_norm, annot=True, fmt='.2%', cmap='YlOrRd')\n",
    "plt.title('Distribuzione IncidentGrade per Cluster')\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('IncidentGrade')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1576403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metriche di confronto\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_train)\n",
    "\n",
    "ari = adjusted_rand_score(y_encoded, cluster_labels)\n",
    "nmi = normalized_mutual_info_score(y_encoded, cluster_labels)\n",
    "\n",
    "print(\"\\nMetriche di accordo con IncidentGrade:\")\n",
    "print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
    "print(f\"  Normalized Mutual Information: {nmi:.4f}\")\n",
    "print(\"\\n(Valori vicini a 1 = alta concordanza, 0 = casuale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7fcfe",
   "metadata": {},
   "source": [
    "## 7. Analisi Caratteristiche Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiungi cluster labels al dataframe\n",
    "X_with_clusters = X_train.copy()\n",
    "X_with_clusters['Cluster'] = cluster_labels\n",
    "\n",
    "# Statistiche per cluster\n",
    "cluster_stats = X_with_clusters.groupby('Cluster').agg(['mean', 'std'])\n",
    "\n",
    "print(\"Statistiche features per cluster (prime 5 features):\")\n",
    "print(cluster_stats.iloc[:, :10].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features più discriminanti tra cluster\n",
    "cluster_means = X_with_clusters.groupby('Cluster').mean()\n",
    "feature_variance = cluster_means.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 features più discriminanti tra cluster:\")\n",
    "print(feature_variance.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2f2a4",
   "metadata": {},
   "source": [
    "## 8. Salvataggio Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e757cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea cartella per K-Means\n",
    "model_dir = '../models/kmeans'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Salva modello\n",
    "with open(f'{model_dir}/model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "# Salva scaler\n",
    "with open(f'{model_dir}/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Salva PCA\n",
    "with open(f'{model_dir}/pca.pkl', 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "# Salva cluster assignments\n",
    "pd.DataFrame({\n",
    "    'cluster': cluster_labels,\n",
    "    'incident_grade': y_train.values\n",
    "}).to_csv(f'{model_dir}/cluster_assignments.csv', index=False)\n",
    "\n",
    "# Salva feature importance (varianza tra cluster)\n",
    "pd.DataFrame({\n",
    "    'Feature': feature_variance.index,\n",
    "    'Variance': feature_variance.values\n",
    "}).to_csv(f'{model_dir}/feature_discrimination.csv', index=False)\n",
    "\n",
    "# Salva metriche\n",
    "metrics = {\n",
    "    'model_name': 'K-Means',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'n_clusters': int(n_clusters),\n",
    "    'inertia': float(kmeans.inertia_),\n",
    "    'silhouette_score': float(silhouette_score(X_scaled, cluster_labels)),\n",
    "    'adjusted_rand_index': float(ari),\n",
    "    'normalized_mutual_info': float(nmi),\n",
    "    'n_samples': int(len(X_train)),\n",
    "    'n_features': int(X_train.shape[1]),\n",
    "    'pca_variance_explained': float(pca.explained_variance_ratio_.sum()),\n",
    "    'cluster_sizes': pd.Series(cluster_labels).value_counts().sort_index().to_dict()\n",
    "}\n",
    "\n",
    "with open(f'{model_dir}/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Modello salvato in {model_dir}/\")\n",
    "print(\"  - model.pkl\")\n",
    "print(\"  - scaler.pkl\")\n",
    "print(\"  - pca.pkl\")\n",
    "print(\"  - cluster_assignments.csv\")\n",
    "print(\"  - feature_discrimination.csv\")\n",
    "print(\"  - metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2884fa",
   "metadata": {},
   "source": [
    "## 9. Riepilogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839cdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RIEPILOGO FINALE - K-MEANS CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDATASET:\")\n",
    "print(f\"  Samples: {len(X_train):,}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nCLUSTERING:\")\n",
    "print(f\"  Numero cluster: {n_clusters}\")\n",
    "print(f\"  Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"  Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.4f}\")\n",
    "\n",
    "print(f\"\\nCONFRONTO CON INCIDENTGRADE:\")\n",
    "print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
    "print(f\"  Normalized Mutual Info: {nmi:.4f}\")\n",
    "\n",
    "print(f\"\\nDISTRIBUZIONE CLUSTER:\")\n",
    "for cluster_id in range(n_clusters):\n",
    "    count = (cluster_labels == cluster_id).sum()\n",
    "    pct = count / len(cluster_labels) * 100\n",
    "    print(f\"  Cluster {cluster_id}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
