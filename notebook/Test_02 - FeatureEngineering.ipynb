{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a6ec63",
   "metadata": {},
   "source": [
    "# Feature Engineering v2 - GUIDE Dataset\n",
    "\n",
    "**Obiettivo:** Preparare il dataset GUIDE per ML con gestione integrata delle MITRE Techniques.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Caricamento e pulizia\n",
    "2. Processing MITRE Techniques (normalizzazione + one-hot encoding)\n",
    "3. Features temporali\n",
    "4. Aggregazione Evidence → Incident\n",
    "5. Train/Test split e salvataggio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165a97f",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "db59c699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:07:27.125820Z",
     "start_time": "2025-10-29T12:07:27.026523Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerie importate con successo!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie importate con successo!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "6857964e",
   "metadata": {},
   "source": [
    "## 2. Caricamento e Pulizia"
   ]
  },
  {
   "cell_type": "code",
   "id": "4200be52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:07:49.669971Z",
     "start_time": "2025-10-29T12:07:29.448896Z"
    }
   },
   "source": [
    "print(\"Caricamento dataset...\")\n",
    "df = pd.read_csv('../data/GUIDE_Train.csv')\n",
    "\n",
    "print(f\"Dataset caricato: {df.shape[0]:,} righe, {df.shape[1]} colonne\")\n",
    "\n",
    "# Rimuovi record senza target\n",
    "df = df[df['IncidentGrade'].notna()].copy()\n",
    "\n",
    "# Rimuovi colonne con >97% missing\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "cols_to_drop = missing_pct[missing_pct > 97].index.tolist()\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Rimuovi colonne geografiche, tecniche e di dettaglio non utili\n",
    "# (alta cardinalità, dati anonimizzati o troppo specifici)\n",
    "cols_to_remove = [\n",
    "    'State', 'City', 'CountryCode',\n",
    "    'OSFamily', 'OSVersion', \n",
    "    'DeviceId', 'DeviceName',\n",
    "    'Sha256', 'FileName', 'FolderPath',\n",
    "    'AccountObjectId', 'AccountName', 'AccountSid', 'AccountUpn',\n",
    "    'IpAddress', 'Url', 'NetworkMessageId', 'EmailClusterId',\n",
    "    'RegistryKey', 'RegistryValueName', 'RegistryValueData',\n",
    "    'ApplicationId', 'ApplicationName', 'OAuthApplicationId',\n",
    "    'ThreatFamily', 'ResourceIdName', 'ResourceType', 'Roles'\n",
    "]\n",
    "\n",
    "# Rimuovi solo quelle che esistono nel dataframe\n",
    "cols_existing = [col for col in cols_to_remove if col in df.columns]\n",
    "if cols_existing:\n",
    "    df = df.drop(columns=cols_existing)\n",
    "    print(f\"Colonne dettaglio rimosse: {len(cols_existing)}\")\n",
    "\n",
    "# Rimuovi duplicati\n",
    "df = df.drop_duplicates(subset=['Id'], keep='first')\n",
    "\n",
    "print(f\"Dimensioni dopo pulizia: {df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset...\n",
      "Dataset caricato: 9,516,837 righe, 45 colonne\n",
      "Colonne dettaglio rimosse: 24\n",
      "Dimensioni dopo pulizia: (707108, 14)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "7985b19b",
   "metadata": {},
   "source": [
    "## 3. Processing MITRE Techniques"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f876c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:07:51.806109Z",
     "start_time": "2025-10-29T12:07:51.560218Z"
    }
   },
   "source": [
    "# Normalizza codici MITRE\n",
    "def normalize_mitre(technique):\n",
    "    if pd.isna(technique):\n",
    "        return 'unknown'\n",
    "    techniques = str(technique).split(';')\n",
    "    normalized = []\n",
    "    for t in techniques:\n",
    "        t = t.strip()\n",
    "        if not t.startswith('T') and t != 'unknown':\n",
    "            t = 'T' + t\n",
    "        if '.' in t and t != 'unknown':\n",
    "            t = t.split('.')[0]\n",
    "        normalized.append(t)\n",
    "    # Rimuovi duplicati e ordina\n",
    "    return ';'.join(sorted(set(normalized)))\n",
    "\n",
    "df['MitreTechniques_normalized'] = df['MitreTechniques'].apply(normalize_mitre)\n",
    "\n",
    "# Conta occorrenze tecniche\n",
    "all_techniques = []\n",
    "for techniques in df['MitreTechniques_normalized']:\n",
    "    all_techniques.extend(techniques.split(';'))\n",
    "technique_counts = Counter(all_techniques)\n",
    "\n",
    "# Seleziona tecniche frequenti (>0.5% del dataset)\n",
    "min_occurrences = len(df) * 0.005\n",
    "frequent_techniques = [tech for tech, count in technique_counts.items() \n",
    "                      if count >= min_occurrences and tech != 'unknown']\n",
    "\n",
    "print(f\"Tecniche frequenti selezionate: {len(frequent_techniques)}\")\n",
    "print(f\"Top 10: {sorted(technique_counts.items(), key=lambda x: x[1], reverse=True)[:10]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tecniche frequenti selezionate: 5\n",
      "Top 10: [('unknown', 461504), ('T1078', 99734), ('T1566', 92722), ('T1110', 11447), ('T1059', 4983), ('T1003', 3739), ('T1559', 3376), ('T1106', 3312), ('T1087', 3104), ('T1485', 3050)]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b51c54c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:07:57.212390Z",
     "start_time": "2025-10-29T12:07:55.917590Z"
    }
   },
   "source": [
    "# One-hot encoding delle tecniche MITRE a livello Evidence\n",
    "def encode_mitre(techniques_str, frequent_techs):\n",
    "    techniques = set(techniques_str.split(';'))\n",
    "    features = {f'MITRE_{tech}': 0 for tech in frequent_techs}\n",
    "    features['MITRE_unknown'] = 1 if 'unknown' in techniques else 0\n",
    "    features['MITRE_n_rare'] = 0\n",
    "    \n",
    "    for tech in techniques:\n",
    "        if tech in frequent_techs:\n",
    "            features[f'MITRE_{tech}'] = 1\n",
    "        elif tech != 'unknown':\n",
    "            features['MITRE_n_rare'] += 1\n",
    "    \n",
    "    features['MITRE_n_rare'] = min(features['MITRE_n_rare'], 5)\n",
    "    return features\n",
    "\n",
    "mitre_encoded = pd.DataFrame([\n",
    "    encode_mitre(tech, frequent_techniques) \n",
    "    for tech in df['MitreTechniques_normalized']\n",
    "])\n",
    "\n",
    "# Aggiungi al dataframe principale\n",
    "df = pd.concat([df, mitre_encoded], axis=1)\n",
    "\n",
    "print(f\"Features MITRE create: {mitre_encoded.shape[1]}\")\n",
    "print(f\"Shape dataset: {df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features MITRE create: 7\n",
      "Shape dataset: (1173396, 22)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "c43a5e09",
   "metadata": {},
   "source": [
    "## 4. Features Temporali"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5708d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:07:59.377458Z",
     "start_time": "2025-10-29T12:07:59.084677Z"
    }
   },
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
    "df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "print(\"Features temporali create\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features temporali create\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "f7fdae4e",
   "metadata": {},
   "source": [
    "## 5. Aggregazione a Livello Incident"
   ]
  },
  {
   "cell_type": "code",
   "id": "90587b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:08:42.454960Z",
     "start_time": "2025-10-29T12:08:04.544118Z"
    }
   },
   "source": [
    "def get_mode(x):\n",
    "    mode = x.mode()\n",
    "    return mode[0] if len(mode) > 0 else x.iloc[0] if len(x) > 0 else None\n",
    "\n",
    "# Prepara aggregazioni\n",
    "agg_dict = {\n",
    "    'IncidentGrade': 'first',\n",
    "    'AlertId': 'nunique',\n",
    "    'Id': 'count',\n",
    "    'EntityType': 'nunique',\n",
    "    'EvidenceRole': 'nunique',\n",
    "    'Category': get_mode,\n",
    "    'Hour': ['min', 'max', 'mean'],\n",
    "    'DayOfWeek': get_mode,\n",
    "    'IsWeekend': 'max',\n",
    "    'Timestamp': ['min', 'max'],\n",
    "    'SuspicionLevel': lambda x: x.notna().sum(),\n",
    "    'LastVerdict': lambda x: x.notna().sum(),\n",
    "}\n",
    "\n",
    "# Aggiungi colonne MITRE alle aggregazioni (somma per incident)\n",
    "mitre_cols = [col for col in df.columns if col.startswith('MITRE_')]\n",
    "for col in mitre_cols:\n",
    "    agg_dict[col] = 'sum'\n",
    "\n",
    "# Esegui aggregazione\n",
    "incident_agg = df.groupby('IncidentId').agg(agg_dict).reset_index()\n",
    "\n",
    "# Flatten colonne multi-livello\n",
    "incident_agg.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col \n",
    "                        for col in incident_agg.columns.values]\n",
    "\n",
    "print(f\"Dataset aggregato: {incident_agg.shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset aggregato: (448901, 23)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "1ca54f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:08:52.651411Z",
     "start_time": "2025-10-29T12:08:52.619366Z"
    }
   },
   "source": [
    "# Calcola durata e rinomina colonne\n",
    "incident_agg['Duration_seconds'] = (\n",
    "    pd.to_datetime(incident_agg['Timestamp_max']) - \n",
    "    pd.to_datetime(incident_agg['Timestamp_min'])\n",
    ").dt.total_seconds()\n",
    "\n",
    "rename_map = {\n",
    "    'AlertId_nunique': 'NumAlerts',\n",
    "    'Id_count': 'NumEvidences',\n",
    "    'EntityType_nunique': 'NumEntityTypes',\n",
    "    'EvidenceRole_nunique': 'NumEvidenceRoles',\n",
    "    'Hour_min': 'Hour_First',\n",
    "    'Hour_max': 'Hour_Last',\n",
    "    'Hour_mean': 'Hour_Avg',\n",
    "    'SuspicionLevel_<lambda>': 'NumWithSuspicion',\n",
    "    'LastVerdict_<lambda>': 'NumWithVerdict',\n",
    "    'IncidentGrade_first': 'IncidentGrade',\n",
    "}\n",
    "\n",
    "incident_agg = incident_agg.rename(columns=rename_map)\n",
    "incident_agg = incident_agg.drop(columns=['Timestamp_min', 'Timestamp_max'], errors='ignore')\n",
    "\n",
    "print(f\"Features finali: {incident_agg.shape[1] - 2}\")  # -2 per ID e target"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features finali: 20\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "7fdc4166",
   "metadata": {},
   "source": [
    "## 6. Encoding Categorici e Split"
   ]
  },
  {
   "cell_type": "code",
   "id": "017b455f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:08:57.201617Z",
     "start_time": "2025-10-29T12:08:57.136339Z"
    }
   },
   "source": [
    "# Separa features e target\n",
    "X = incident_agg.drop(columns=['IncidentId', 'IncidentGrade'])\n",
    "y = incident_agg['IncidentGrade']\n",
    "\n",
    "# Identifica colonne categoriche\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Riduzione cardinalità alte\n",
    "for col in categorical_cols:\n",
    "    if X[col].nunique() > 100:\n",
    "        top_values = X[col].value_counts().head(50).index\n",
    "        X[col] = X[col].apply(lambda x: x if x in top_values else 'Other')\n",
    "\n",
    "# Label encoding\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Gestisci missing\n",
    "X = X.fillna(-999)\n",
    "\n",
    "print(f\"Features finali: {X.shape}\")\n",
    "print(f\"Target: {y.shape}\")\n",
    "print(f\"\\nDistribuzione target:\\n{y.value_counts(normalize=True)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features finali: (448901, 20)\n",
      "Target: (448901,)\n",
      "\n",
      "Distribuzione target:\n",
      "IncidentGrade\n",
      "BenignPositive    0.485922\n",
      "FalsePositive     0.301086\n",
      "TruePositive      0.212991\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5acb47c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:08:59.367358Z",
     "start_time": "2025-10-29T12:08:59.172451Z"
    }
   },
   "source": [
    "# Split stratificato\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"\\nDistribuzione y_train:\\n{y_train.value_counts(normalize=True)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (314230, 20)\n",
      "X_test: (134671, 20)\n",
      "\n",
      "Distribuzione y_train:\n",
      "IncidentGrade\n",
      "BenignPositive    0.485921\n",
      "FalsePositive     0.301088\n",
      "TruePositive      0.212990\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "7477ba0a",
   "metadata": {},
   "source": [
    "## 7. Salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "id": "5919b447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:09:03.638939Z",
     "start_time": "2025-10-29T12:09:01.036168Z"
    }
   },
   "source": [
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False, header=['IncidentGrade'])\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False, header=['IncidentGrade'])\n",
    "\n",
    "with open('../data/processed/label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "incident_agg.to_csv('../data/processed/incident_features.csv', index=False)\n",
    "\n",
    "print(\"Dataset salvati in ../data/processed/\")\n",
    "print(f\"  - X_train.csv: {X_train.shape}\")\n",
    "print(f\"  - X_test.csv: {X_test.shape}\")\n",
    "print(f\"  - Features totali: {X_train.shape[1]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset salvati in ../data/processed/\n",
      "  - X_train.csv: (314230, 20)\n",
      "  - X_test.csv: (134671, 20)\n",
      "  - Features totali: 20\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
