{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a22bece",
   "metadata": {},
   "source": [
    "# Feature Engineering v3 - GUIDE Dataset\n",
    "\n",
    "**Obiettivo:** Feature engineering avanzato con target binario e encoding ottimizzato.\n",
    "\n",
    "**Differenze rispetto a v2:**\n",
    "- Target binario: BinaryIncidentGrade (1=TruePositive, 0=FalsePositive/BenignPositive)\n",
    "- SmoothedRisk per AlertTitle (Bayesian smoothing)\n",
    "- GeoLoc_freq invece di encoding geografico standard\n",
    "- Frequency encoding per colonne ad alta cardinalità\n",
    "- MITRE top 30 tecniche\n",
    "- One-hot encoding selettivo solo per SuspicionLevel e EvidenceRole\n",
    "\n",
    "**Pipeline:**\n",
    "1. Caricamento e pulizia\n",
    "2. Target binario\n",
    "3. SmoothedRisk per AlertTitle\n",
    "4. GeoLoc_freq\n",
    "5. Features temporali\n",
    "6. Frequency encoding categorie\n",
    "7. One-hot encoding selettivo\n",
    "8. Processing MITRE (top 30)\n",
    "9. Aggregazione Evidence → Incident\n",
    "10. Train/Test split e salvataggio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61097e4f",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e82f83e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:53:32.792600Z",
     "start_time": "2025-10-29T12:53:32.642338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie importate con successo!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerie importate con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9ee3e",
   "metadata": {},
   "source": [
    "## 2. Caricamento e Pulizia"
   ]
  },
  {
   "cell_type": "code",
   "id": "393a20e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:57:34.920762Z",
     "start_time": "2025-10-29T12:57:16.689490Z"
    }
   },
   "source": [
    "print(\"Caricamento dataset...\")\n",
    "df = pd.read_csv('../data/GUIDE_Train.csv')\n",
    "\n",
    "print(f\"Dataset caricato: {df.shape[0]:,} righe, {df.shape[1]} colonne\")\n",
    "\n",
    "# Rimuovi record senza target\n",
    "df = df[df['IncidentGrade'].notna()].copy()\n",
    "\n",
    "# Rimuovi duplicati\n",
    "#df = df.drop_duplicates(subset=['Id'], keep='first')\n",
    "\n",
    "print(f\"Dimensioni dopo pulizia: {df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset...\n",
      "Dataset caricato: 9,516,837 righe, 45 colonne\n",
      "Dimensioni dopo pulizia: (9465497, 45)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "efe90533",
   "metadata": {},
   "source": [
    "## 3. Target Binario"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab1e18d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:58:22.019087Z",
     "start_time": "2025-10-29T12:58:20.903362Z"
    }
   },
   "source": [
    "# Crea target binario: 1 = TruePositive, 0 = FalsePositive/BenignPositive\n",
    "df['BinaryIncidentGrade'] = df['IncidentGrade'].apply(\n",
    "    lambda x: 1 if x == 'TruePositive' else 0\n",
    ")\n",
    "\n",
    "counts = df['BinaryIncidentGrade'].value_counts()\n",
    "percentages = df['BinaryIncidentGrade'].value_counts(normalize=True).mul(100).round(2)\n",
    "\n",
    "result = pd.DataFrame({'Count': counts, 'Percentage': percentages})\n",
    "print(\"\\nDistribuzione Target Binario:\")\n",
    "print(result)\n",
    "print(f\"\\nClass imbalance ratio: {counts[0]/counts[1]:.2f}:1\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuzione Target Binario:\n",
      "                       Count  Percentage\n",
      "BinaryIncidentGrade                     \n",
      "0                    6142784        64.9\n",
      "1                    3322713        35.1\n",
      "\n",
      "Class imbalance ratio: 1.85:1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "291d34d2",
   "metadata": {},
   "source": [
    "## 4. SmoothedRisk per AlertTitle\n",
    "\n",
    "**Wilson/Bayes Smoothing:** Corregge la media quando abbiamo pochi esempi.  \n",
    "Con solo 2 esempi e 100% risk, lo smoothing porta il valore verso 0.5 per riflettere l'incertezza."
   ]
  },
  {
   "cell_type": "code",
   "id": "f67da2f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:58:26.022615Z",
     "start_time": "2025-10-29T12:58:23.980356Z"
    }
   },
   "source": [
    "# Calcola risk grezzo per AlertTitle\n",
    "alert_risk = df.groupby('AlertTitle')['BinaryIncidentGrade'].mean()\n",
    "alert_count = df.groupby('AlertTitle')['BinaryIncidentGrade'].sum()\n",
    "\n",
    "alert_summary = pd.DataFrame({\n",
    "    'Risk': alert_risk,\n",
    "    'Count': alert_count\n",
    "})\n",
    "\n",
    "# Bayesian smoothing\n",
    "alpha = 2\n",
    "beta = 2\n",
    "\n",
    "alert_summary['SmoothedRisk'] = (\n",
    "    alert_summary['Risk'] * alert_summary['Count'] + alpha\n",
    ") / (alert_summary['Count'] + alpha + beta)\n",
    "\n",
    "# Merge nel dataframe\n",
    "df = df.merge(\n",
    "    alert_summary[['SmoothedRisk']], \n",
    "    left_on='AlertTitle', \n",
    "    right_index=True, \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"SmoothedRisk creato per {len(alert_summary)} AlertTitle univoci\")\n",
    "print(f\"\\nStatistiche SmoothedRisk:\")\n",
    "print(df['SmoothedRisk'].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoothedRisk creato per 79952 AlertTitle univoci\n",
      "\n",
      "Statistiche SmoothedRisk:\n",
      "count    9.465497e+06\n",
      "mean     4.901028e-01\n",
      "std      2.674729e-01\n",
      "min      1.622586e-02\n",
      "25%      2.835467e-01\n",
      "50%      5.000000e-01\n",
      "75%      7.540790e-01\n",
      "max      9.999631e-01\n",
      "Name: SmoothedRisk, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "4200f2ae",
   "metadata": {},
   "source": [
    "## 5. GeoLoc_freq\n",
    "\n",
    "Frequenza normalizzata della combinazione `CountryCode_State_City`.  \n",
    "Utile per identificare location rare o pattern regionali."
   ]
  },
  {
   "cell_type": "code",
   "id": "be7e7b61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:58:37.391870Z",
     "start_time": "2025-10-29T12:58:32.149315Z"
    }
   },
   "source": [
    "# Crea identificatore geografico\n",
    "df['GeoLoc'] = (\n",
    "    df['CountryCode'].astype(str) + \"_\" + \n",
    "    df['State'].astype(str) + \"_\" + \n",
    "    df['City'].astype(str)\n",
    ")\n",
    "\n",
    "# Calcola frequenza normalizzata\n",
    "geo_freq = df['GeoLoc'].value_counts(normalize=True)\n",
    "df['GeoLoc_freq'] = df['GeoLoc'].map(geo_freq)\n",
    "\n",
    "# Drop colonne geografiche originali\n",
    "df.drop(columns=['CountryCode', 'State', 'City', 'GeoLoc'], inplace=True)\n",
    "\n",
    "print(\"GeoLoc_freq creato\")\n",
    "print(df['GeoLoc_freq'].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoLoc_freq creato\n",
      "count    9.465497e+06\n",
      "mean     8.479237e-01\n",
      "std      2.481550e-01\n",
      "min      1.056469e-07\n",
      "25%      9.207343e-01\n",
      "50%      9.207343e-01\n",
      "75%      9.207343e-01\n",
      "max      9.207343e-01\n",
      "Name: GeoLoc_freq, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d18b7b15",
   "metadata": {},
   "source": [
    "## 6. Features Temporali"
   ]
  },
  {
   "cell_type": "code",
   "id": "dfb94a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:58:43.153914Z",
     "start_time": "2025-10-29T12:58:39.359829Z"
    }
   },
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df['month'] = df['Timestamp'].dt.month\n",
    "df['hour'] = df['Timestamp'].dt.hour\n",
    "df['weekday'] = df['Timestamp'].dt.weekday + 1\n",
    "df['IsWeekend'] = (df['Timestamp'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Conserva Timestamp per aggregazione, poi rimuoveremo\n",
    "print(\"Features temporali create: month, hour, weekday, IsWeekend\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features temporali create: month, hour, weekday, IsWeekend\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "e92c89dd",
   "metadata": {},
   "source": [
    "## 7. Gestione Missing e Frequency Encoding\n",
    "\n",
    "**Frequency encoding** per colonne ad alta cardinalità evita curse of dimensionality.  \n",
    "Fornisce un segnale semplice e limitato."
   ]
  },
  {
   "cell_type": "code",
   "id": "e45e1f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:58:54.548389Z",
     "start_time": "2025-10-29T12:58:53.341826Z"
    }
   },
   "source": [
    "# Fill missing values\n",
    "df['Roles'] = df['Roles'].fillna('missing')\n",
    "df['ActionGrouped'] = df['ActionGrouped'].fillna('Missing')\n",
    "df['SuspicionLevel'] = df['SuspicionLevel'].fillna('Missing')\n",
    "df['LastVerdict'] = df['LastVerdict'].fillna('Missing')\n",
    "\n",
    "# Group rare verdicts (< 100 occorrenze)\n",
    "verdict_counts = df['LastVerdict'].value_counts()\n",
    "rare_verdicts = verdict_counts[verdict_counts < 100].index\n",
    "df['LastVerdict'] = df['LastVerdict'].replace(rare_verdicts, 'Other')\n",
    "\n",
    "print(\"Missing values gestiti\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values gestiti\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "a551866f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:59:09.488201Z",
     "start_time": "2025-10-29T12:58:58.959498Z"
    }
   },
   "source": [
    "# Frequency encoding per colonne ad alta cardinalità\n",
    "freq_encode_cols = [\n",
    "    'ThreatFamily', 'AntispamDirection', 'ActionGranular',\n",
    "    'LastVerdict', 'ResourceType', 'Roles', 'ActionGrouped', \n",
    "    'EntityType', 'Category'\n",
    "]\n",
    "\n",
    "for col in freq_encode_cols:\n",
    "    if col in df.columns:\n",
    "        # Fill missing\n",
    "        df[col] = df[col].fillna('Missing')\n",
    "        \n",
    "        # Frequency encode\n",
    "        freq = df[col].value_counts(normalize=True)\n",
    "        df[f\"{col}_freq\"] = df[col].map(freq)\n",
    "        \n",
    "        # Drop original\n",
    "        df.drop(columns=col, inplace=True)\n",
    "        print(f\"  {col} -> {col}_freq\")\n",
    "\n",
    "print(f\"\\nFrequency encoding completato per {len(freq_encode_cols)} colonne\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ThreatFamily -> ThreatFamily_freq\n",
      "  AntispamDirection -> AntispamDirection_freq\n",
      "  ActionGranular -> ActionGranular_freq\n",
      "  LastVerdict -> LastVerdict_freq\n",
      "  ResourceType -> ResourceType_freq\n",
      "  Roles -> Roles_freq\n",
      "  ActionGrouped -> ActionGrouped_freq\n",
      "  EntityType -> EntityType_freq\n",
      "  Category -> Category_freq\n",
      "\n",
      "Frequency encoding completato per 9 colonne\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "f9cdda5d",
   "metadata": {},
   "source": [
    "## 8. One-Hot Encoding Selettivo\n",
    "\n",
    "Solo per `SuspicionLevel` e `EvidenceRole` (bassa cardinalità e forte segnale)."
   ]
  },
  {
   "cell_type": "code",
   "id": "23ee11a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:59:32.211105Z",
     "start_time": "2025-10-29T12:59:32.208273Z"
    }
   },
   "source": [
    "onehot_cols = ['SuspicionLevel', 'EvidenceRole']\n",
    "\n",
    "for col in onehot_cols:\n",
    "    if col in df.columns:\n",
    "        # Fill missing\n",
    "        df[col] = df[col].fillna('Missing')\n",
    "        \n",
    "        # Group rare categories (< 100 occorrenze)\n",
    "        counts = df[col].value_counts()\n",
    "        rare = counts[counts < 100].index\n",
    "        df[col] = df[col].replace(rare, 'Other')\n",
    "        \n",
    "        # One-hot encode (drop_first per evitare multicollinearità)\n",
    "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "        print(f\"  {col} -> one-hot encoded\")\n",
    "\n",
    "print(\"\\nOne-hot encoding completato\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-hot encoding completato\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "29dbbe6d",
   "metadata": {},
   "source": [
    "## 9. Processing MITRE Techniques (Top 30)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc1be622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:59:44.813794Z",
     "start_time": "2025-10-29T12:59:35.746852Z"
    }
   },
   "source": [
    "# Step 1: Split semicolon-separated string\n",
    "df['MitreList'] = df['MitreTechniques'].apply(\n",
    "    lambda x: x.split(';') if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "# Step 2: Identifica top 30 tecniche\n",
    "all_techs = [tech for sublist in df['MitreList'] for tech in sublist]\n",
    "top_techs = [tech for tech, _ in Counter(all_techs).most_common(30)]\n",
    "top_tech_set = set(top_techs)\n",
    "\n",
    "print(f\"Top 30 MITRE techniques selezionate\")\n",
    "print(f\"Top 10: {Counter(all_techs).most_common(10)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 MITRE techniques selezionate\n",
      "Top 10: [('T1078', 1467176), ('T1078.004', 1357022), ('T1566.002', 815149), ('T1566', 659978), ('T1110', 189348), ('T1133', 177415), ('T1566.001', 140374), ('T1110.003', 108831), ('T1110.001', 107510), ('T1071', 96343)]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "445b8a49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:00:15.499321Z",
     "start_time": "2025-10-29T12:59:48.363701Z"
    }
   },
   "source": [
    "# Step 3: Filtra ogni lista per includere solo top techniques\n",
    "df['FilteredMitreList'] = df['MitreList'].apply(\n",
    "    lambda x: [tech for tech in x if tech in top_tech_set]\n",
    ")\n",
    "\n",
    "# Step 4: One-hot encode con MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=top_techs)\n",
    "tech_matrix = pd.DataFrame(\n",
    "    mlb.fit_transform(df['FilteredMitreList']),\n",
    "    columns=mlb.classes_, \n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Step 5: Merge e drop colonne originali\n",
    "df = pd.concat([df, tech_matrix], axis=1)\n",
    "df.drop(columns=['MitreTechniques', 'MitreList', 'FilteredMitreList'], inplace=True)\n",
    "\n",
    "print(f\"\\nMITRE features create: {tech_matrix.shape[1]}\")\n",
    "print(f\"Shape dataset: {df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MITRE features create: 30\n",
      "Shape dataset: (9465497, 79)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "72a55b2f",
   "metadata": {},
   "source": [
    "## 10. Aggregazione a Livello Incident"
   ]
  },
  {
   "cell_type": "code",
   "id": "b20d7544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:00:28.062990Z",
     "start_time": "2025-10-29T13:00:28.057622Z"
    }
   },
   "source": [
    "def get_mode(x):\n",
    "    mode = x.mode()\n",
    "    return mode[0] if len(mode) > 0 else x.iloc[0] if len(x) > 0 else None\n",
    "\n",
    "# Prepara aggregazioni\n",
    "agg_dict = {\n",
    "    'BinaryIncidentGrade': 'first',\n",
    "    'IncidentGrade': 'first',\n",
    "    'AlertId': 'nunique',\n",
    "    'Id': 'count',\n",
    "    'SmoothedRisk': 'mean',\n",
    "    'GeoLoc_freq': 'mean',\n",
    "    'hour': ['min', 'max', 'mean'],\n",
    "    'month': get_mode,\n",
    "    'weekday': get_mode,\n",
    "    'IsWeekend': 'max',\n",
    "    'Timestamp': ['min', 'max'],\n",
    "}\n",
    "\n",
    "# Aggiungi frequency-encoded columns (media)\n",
    "freq_cols = [col for col in df.columns if col.endswith('_freq') and col not in ['GeoLoc_freq']]\n",
    "for col in freq_cols:\n",
    "    agg_dict[col] = 'mean'\n",
    "\n",
    "# Aggiungi one-hot encoded columns (somma)\n",
    "onehot_cols_created = [col for col in df.columns if col.startswith(('SuspicionLevel_', 'EvidenceRole_'))]\n",
    "for col in onehot_cols_created:\n",
    "    agg_dict[col] = 'sum'\n",
    "\n",
    "# Aggiungi MITRE columns (somma)\n",
    "mitre_cols = [col for col in df.columns if col.startswith('T') and len(col) <= 6]\n",
    "for col in mitre_cols:\n",
    "    agg_dict[col] = 'sum'\n",
    "\n",
    "print(f\"Aggregazioni preparate per {len(agg_dict)} features\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregazioni preparate per 43 features\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "82d61cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:00:59.150695Z",
     "start_time": "2025-10-29T13:00:35.515904Z"
    }
   },
   "source": [
    "# Esegui aggregazione\n",
    "incident_agg = df.groupby('IncidentId').agg(agg_dict).reset_index()\n",
    "\n",
    "# Flatten colonne multi-livello\n",
    "incident_agg.columns = [\n",
    "    '_'.join(col).strip('_') if isinstance(col, tuple) else col \n",
    "    for col in incident_agg.columns.values\n",
    "]\n",
    "\n",
    "print(f\"Dataset aggregato: {incident_agg.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset aggregato: (448901, 47)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "cfef6c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:01:09.297881Z",
     "start_time": "2025-10-29T13:01:09.174283Z"
    }
   },
   "source": [
    "# Calcola durata e rinomina colonne\n",
    "incident_agg['Duration_seconds'] = (\n",
    "    pd.to_datetime(incident_agg['Timestamp_max']) - \n",
    "    pd.to_datetime(incident_agg['Timestamp_min'])\n",
    ").dt.total_seconds()\n",
    "\n",
    "rename_map = {\n",
    "    'AlertId_nunique': 'NumAlerts',\n",
    "    'Id_count': 'NumEvidences',\n",
    "    'SmoothedRisk_mean': 'SmoothedRisk_avg',\n",
    "    'GeoLoc_freq_mean': 'GeoLoc_freq_avg',\n",
    "    'hour_min': 'Hour_First',\n",
    "    'hour_max': 'Hour_Last',\n",
    "    'hour_mean': 'Hour_Avg',\n",
    "    'BinaryIncidentGrade_first': 'BinaryIncidentGrade',\n",
    "    'IncidentGrade_first': 'IncidentGrade',\n",
    "}\n",
    "\n",
    "incident_agg = incident_agg.rename(columns=rename_map)\n",
    "incident_agg = incident_agg.drop(columns=['Timestamp_min', 'Timestamp_max'], errors='ignore')\n",
    "\n",
    "print(f\"Features finali: {incident_agg.shape[1] - 3}\")  # -3 per ID e 2 target"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features finali: 43\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "dfee85ee",
   "metadata": {},
   "source": [
    "## 11. Preparazione per Modeling"
   ]
  },
  {
   "cell_type": "code",
   "id": "a184f611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:01:13.579750Z",
     "start_time": "2025-10-29T13:01:13.557895Z"
    }
   },
   "source": [
    "# Separa features e target\n",
    "X = incident_agg.drop(columns=['IncidentId', 'BinaryIncidentGrade', 'IncidentGrade'])\n",
    "y = incident_agg['BinaryIncidentGrade']\n",
    "\n",
    "# Gestisci missing (se presenti)\n",
    "X = X.fillna(-999)\n",
    "\n",
    "print(f\"Features finali: {X.shape}\")\n",
    "print(f\"Target: {y.shape}\")\n",
    "print(f\"\\nDistribuzione target binario:\")\n",
    "print(y.value_counts(normalize=True))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features finali: (448901, 43)\n",
      "Target: (448901,)\n",
      "\n",
      "Distribuzione target binario:\n",
      "BinaryIncidentGrade\n",
      "0    0.787009\n",
      "1    0.212991\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "3e55e036",
   "metadata": {},
   "source": [
    "## 12. Train/Test Split Stratificato"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f5005cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:01:45.306126Z",
     "start_time": "2025-10-29T13:01:45.215035Z"
    }
   },
   "source": [
    "# Split stratificato (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"\\nDistribuzione y_train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nDistribuzione y_test:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (314230, 43)\n",
      "X_test: (134671, 43)\n",
      "\n",
      "Distribuzione y_train:\n",
      "BinaryIncidentGrade\n",
      "0    0.78701\n",
      "1    0.21299\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribuzione y_test:\n",
      "BinaryIncidentGrade\n",
      "0    0.787007\n",
      "1    0.212993\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "581bd726",
   "metadata": {},
   "source": [
    "## 13. Salvataggio Dataset Processati"
   ]
  },
  {
   "cell_type": "code",
   "id": "64649da9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:01:57.538559Z",
     "start_time": "2025-10-29T13:01:51.260580Z"
    }
   },
   "source": [
    "os.makedirs('../data/processed_v3', exist_ok=True)\n",
    "\n",
    "X_train.to_csv('../data/processed_v3/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed_v3/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed_v3/y_train.csv', index=False, header=['BinaryIncidentGrade'])\n",
    "y_test.to_csv('../data/processed_v3/y_test.csv', index=False, header=['BinaryIncidentGrade'])\n",
    "\n",
    "# Salva anche il dataframe completo aggregato\n",
    "incident_agg.to_csv('../data/processed_v3/incident_features.csv', index=False)\n",
    "\n",
    "print(\"Dataset salvati in ../data/processed_v3/\")\n",
    "print(f\"  - X_train.csv: {X_train.shape}\")\n",
    "print(f\"  - X_test.csv: {X_test.shape}\")\n",
    "print(f\"  - y_train.csv: {y_train.shape}\")\n",
    "print(f\"  - y_test.csv: {y_test.shape}\")\n",
    "print(f\"  - incident_features.csv: {incident_agg.shape}\")\n",
    "print(f\"\\nFeatures totali: {X_train.shape[1]}\")\n",
    "print(f\"Target binario: 0 (FP/BP) vs 1 (TP)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset salvati in ../data/processed_v3/\n",
      "  - X_train.csv: (314230, 43)\n",
      "  - X_test.csv: (134671, 43)\n",
      "  - y_train.csv: (314230,)\n",
      "  - y_test.csv: (134671,)\n",
      "  - incident_features.csv: (448901, 46)\n",
      "\n",
      "Features totali: 43\n",
      "Target binario: 0 (FP/BP) vs 1 (TP)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "2984d0ba",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Features create:**\n",
    "- Target binario (BinaryIncidentGrade)\n",
    "- SmoothedRisk (Bayesian smoothing per AlertTitle)\n",
    "- GeoLoc_freq (frequenza location)\n",
    "- Temporal features (hour, month, weekday, IsWeekend, Duration_seconds)\n",
    "- Frequency encoding per 9 colonne ad alta cardinalità\n",
    "- One-hot encoding per SuspicionLevel e EvidenceRole\n",
    "- MITRE top 30 tecniche (one-hot encoded)\n",
    "- Aggregazioni a livello incident (count, mean, sum, mode)\n",
    "\n",
    "**Pronto per training XGBoost con target binario!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
